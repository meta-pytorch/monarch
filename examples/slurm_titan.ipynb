{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c91f6d2",
   "metadata": {},
   "source": [
    "## Monarch + TorchTitan on SLURM\n",
    "This example notebook demonstrates how you can easily run and iterate on a distributed training job with Monarch and TorchTitan.\n",
    "\n",
    "#### Prerequisites\n",
    "Please make sure your environment is setup for this notebook:\n",
    "1. Install Monarch nightly: https://github.com/meta-pytorch/monarch/blob/main/scripts/install_nightly.py\n",
    "2. Install Titan nightly: https://github.com/pytorch/torchtitan?tab=readme-ov-file#nightly-builds\n",
    "3. Ensure you have a valid Titan model config in the script directory (i.e: https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/train_configs/debug_model.toml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd971d",
   "metadata": {},
   "source": [
    "### 1. Create your SLURM job\n",
    "Configure parameters for your cluster:\n",
    "- num_nodes: Number of nodes to allocate (default: 2)\n",
    "- gpus_per_node: Number of GPUs per node (default: 8)\n",
    "- mesh_name: Name for the mesh (default: \"mesh0\")\n",
    "- time_limit: Maximum job duration (default: \"06:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "from slurm.utils import create_slurm_job, cleanup_job\n",
    "\n",
    "num_nodes = 2  # assign for your system\n",
    "gpus_per_node = 8  # adjust for your hardware\n",
    "mesh_name = \"mesh0\"\n",
    "\n",
    "# Create a SLURM job with N nodes\n",
    "slurm_job = create_slurm_job(\n",
    "    mesh_name,\n",
    "    num_nodes,\n",
    "    gpus_per_node,\n",
    "    # time_limit=\"06:00:00\",  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e41ce",
   "metadata": {},
   "source": [
    "### 2. Define your Titan and cluster parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d51df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "from torchtitan.train import Trainer\n",
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.actor import Actor, current_rank, endpoint\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunParams:\n",
    "    \"\"\"\n",
    "        Parameters for your cluster and training job, adjust as needed\n",
    "    \"\"\"\n",
    "    training_steps: int = 50\n",
    "    model_config = \"debug_model.toml\"\n",
    "    dataset = \"c4\"\n",
    "    num_nodes = num_nodes\n",
    "    gpus_per_node = gpus_per_node\n",
    "\n",
    "\n",
    "class TrainerActor(Actor):\n",
    "    \"\"\"\n",
    "        A simple wrapper class with executes a TorchTitan trainer in a Monarch actor\n",
    "    \"\"\"\n",
    "    def __init__(self, job_config: JobConfig) -> None:\n",
    "        self.job_config = job_config\n",
    "        rank = current_rank().rank\n",
    "        self.uid = f\"[trainer_{rank}]\"\n",
    "\n",
    "    @endpoint\n",
    "    async def start_training(self) -> None:\n",
    "        init_logger()\n",
    "        trainer: Trainer | None = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(self.job_config)\n",
    "            logger.info(f\"{self.uid} initialized successfully and starting training\")\n",
    "            trainer.train()\n",
    "        except Exception:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "            raise\n",
    "        else:\n",
    "            trainer.close()\n",
    "        finally:\n",
    "            torch.distributed.destroy_process_group()\n",
    "            logger.info(f\"{self.uid} trainer cleaned up\")\n",
    "\n",
    "def make_job_config() -> JobConfig:\n",
    "    \"\"\"\n",
    "        Create a job config which is digested by TorchTitan, sourced from RunParams\n",
    "    \"\"\"\n",
    "    data_parallel_shard_degree = RunParams.num_nodes * RunParams.gpus_per_node\n",
    "    output_path = \"./outputs\"\n",
    "\n",
    "    script_dir = globals()['_dh'][0]\n",
    "    default_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.join(script_dir, RunParams.model_config),\n",
    "        \"--model.hf_assets_path\",\n",
    "        os.path.join(script_dir, \"tokenizer\"),\n",
    "        \"--comm.trace_buf_size\",\n",
    "        \"0\",\n",
    "        \"--metrics.log_freq\",\n",
    "        \"1\",\n",
    "        \"--parallelism.data_parallel_shard_degree\",\n",
    "        str(data_parallel_shard_degree),\n",
    "        \"--activation_checkpoint.mode\",\n",
    "        \"full\",\n",
    "        \"--comm.train_timeout_seconds\",\n",
    "        \"60\",\n",
    "        \"--training.steps\",\n",
    "        str(RunParams.training_steps),\n",
    "        \"--training.dataset\",\n",
    "        RunParams.dataset,\n",
    "        \"--job.dump_folder\",\n",
    "        output_path,\n",
    "        \"--metrics.enable_tensorboard\",\n",
    "    ]\n",
    "\n",
    "    config_manager = ConfigManager()\n",
    "    job_config = config_manager.parse_args(default_args)\n",
    "\n",
    "    return job_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04425384",
   "metadata": {},
   "source": [
    "### 3. Execute your training job\n",
    "You can make adjustments and run this on the existing SLURM allocations as many times as you would like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091a7066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached job at path: .monarch/job_state.pkl\n",
      "SLURM job 7748 not found in queue\n",
      "Cached job cannot run this spec, removing cache\n",
      "Cancelled SLURM job 7748\n",
      "Applying current job\n",
      "Submitting SLURM job with 2 nodes\n",
      "SLURM job 7749 submitted. Logs will be written to: /home/mreso/monarch/examples/slurm_7749_monarch_example_1780323.out\n",
      "Saving job to cache at .monarch/job_state.pkl\n",
      "Job has started, connecting to current state\n",
      "SLURM job 7749 is running on 2 nodes: ['slurm-compute-node-074', 'slurm-compute-node-077']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:28) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [6] [titan] 2025-11-15 00:42:38,346 - root - INFO - Starting job: Llama 3 debug training\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:38) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:28) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [2] /home/mreso/miniforge3/envs/monarch/lib/python3.12/site-packages/torch/distributed/device_mesh.py:604: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[8 similar log lines]\u001b[0m [2]   sliced_mesh_layout = self._get_slice_mesh_layout(mesh_dim_names)\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:41) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:38) >>>\u001b[0m\n",
      "\u001b[33m[7 similar log lines]\u001b[0m [4] [titan] 2025-11-15 00:42:38,347 - root - INFO - Starting job: Llama 3 debug training\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [2] [titan] 2025-11-15 00:42:38,121 - root - INFO - Building 1-D device mesh with ['dp_shard'], [8]\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [2] [titan] 2025-11-15 00:42:38,126 - root - INFO - [GC] Initial GC collection took 0.00 seconds\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:41) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:41) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [5] [titan] 2025-11-15 00:42:43,362 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [5] [titan] 2025-11-15 00:42:43,365 - root - INFO - Preparing c4 dataset from allenai/c4\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:44) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:44) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [6] [titan] 2025-11-15 00:42:46,502 - root - INFO - Building llama3 debugmodel with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=256, n_layers=6, n_heads=16, n_kv_heads=None, vocab_size=2048, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, rope_scaling_args=RoPEScalingArgs(scaling_factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192), max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [6] [titan] 2025-11-15 00:42:46,511 - root - INFO - CUDA capacity: NVIDIA GB200 with 184.00GiB memory\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,359 - root - INFO - \u001b[34mModel llama3 debugmodel \u001b[31msize: 6,163,712 total parameters\u001b[39m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,359 - root - INFO - Applied full activation checkpointing to the model\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,389 - root - INFO - Applied FSDP to the model\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,545 - root - INFO - Peak FLOPS used for computing MFU: 2.250e+15\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,546 - root - INFO - CUDA memory usage for model: 0.00GiB(0.00%)\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,547 - root - WARNING - model.safetensors.index.json not found at hf_assets_path: /home/mreso/torchtitan/tests/assets/tokenizer/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,547 - root - INFO - Mixed precision training is handled by fully_shard\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,547 - root - INFO - Trainer is initialized with local batch size 8, global batch size 64, gradient accumulation steps 1, sequence length 2048, total steps 50 (warmup 2)\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,547 - root - INFO - [trainer_1] initialized successfully and starting training\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:45,547 - root - INFO - Training starts at step 1\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [0] [titan] 2025-11-15 00:42:45,594 - root - INFO - TensorBoard logging enabled. Logs will be saved at ./outputs/tb/20251115-0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:41) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1] /home/mreso/miniforge3/envs/monarch/lib/python3.12/site-packages/torch/distributed/device_mesh.py:604: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[8 similar log lines]\u001b[0m [1]   sliced_mesh_layout = self._get_slice_mesh_layout(mesh_dim_names)\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:47) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:47) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:47) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [7] /home/mreso/miniforge3/envs/monarch/lib/python3.12/site-packages/torch/distributed/device_mesh.py:604: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[1 similar log lines]\u001b[0m [7]   sliced_mesh_layout = self._get_slice_mesh_layout(mesh_dim_names)\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:50) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:47) >>>\u001b[0m\n",
      "\u001b[33m[275 similar log lines]\u001b[0m [5] [titan] 2025-11-15 00:42:50,809 - root - INFO - \u001b[31mstep:  1  \u001b[32mloss:  8.0601  \u001b[38;2;180;60;0mgrad_norm:  1.4225  \u001b[38;2;54;234;195mmemory:  0.68GiB(0.37%)  \u001b[34mtps: 3,867  \u001b[36mtflops: 0.28  \u001b[35mmfu: 0.01%\u001b[39m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [5] [titan] 2025-11-15 00:42:50,810 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:00\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:53) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:50) >>>\u001b[0m\n",
      "\u001b[33m[7 similar log lines]\u001b[0m [5] /home/mreso/miniforge3/envs/monarch/lib/python3.12/site-packages/torch/distributed/device_mesh.py:604: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[7 similar log lines]\u001b[0m [5]   sliced_mesh_layout = self._get_slice_mesh_layout(mesh_dim_names)\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:53) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:53) >>>\u001b[0m\n",
      "\u001b[33m[5 similar log lines]\u001b[0m [4] [titan] 2025-11-15 00:42:53,801 - root - INFO - \u001b[31mstep: 35  \u001b[32mloss:  2.8679  \u001b[38;2;180;60;0mgrad_norm:  0.2479  \u001b[38;2;54;234;195mmemory:  0.69GiB(0.37%)  \u001b[34mtps: 189,334  \u001b[36mtflops: 13.55  \u001b[35mmfu: 0.60%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:53) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:53) >>>\u001b[0m\n",
      "\u001b[33m[120 similar log lines]\u001b[0m [1] [titan] 2025-11-15 00:42:52,697 - root - INFO - \u001b[31mstep: 36  \u001b[32mloss:  2.8734  \u001b[38;2;180;60;0mgrad_norm:  0.2411  \u001b[38;2;54;234;195mmemory:  0.69GiB(0.37%)  \u001b[34mtps: 192,839  \u001b[36mtflops: 13.80  \u001b[35mmfu: 0.61%\u001b[39m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [2] [titan] 2025-11-15 00:42:53,894 - root - INFO - [GC] Performing periodic GC collection took 0.04 seconds\n",
      "\u001b[33m[7 similar log lines]\u001b[0m [2] [titan] 2025-11-15 00:42:54,017 - root - INFO - Training completed\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [0] [titan] 2025-11-15 00:42:54,018 - root - INFO - Sleeping 2 seconds for other ranks to complete\n",
      "\u001b[33m[7 similar log lines]\u001b[0m [2] [titan] 2025-11-15 00:42:54,239 - root - INFO - [trainer_2] trainer cleaned up\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:56) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root 2025-11-15 00:42:57 INFO Training completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-11-15 00:42:56) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [0] [titan] 2025-11-15 00:42:56,018 - root - INFO - Training completed\n",
      "\u001b[33m[1 similar log lines]\u001b[0m [0] [titan] 2025-11-15 00:42:56,291 - root - INFO - [trainer_0] trainer cleaned up\n",
      "\u001b[36m<<< Aggregated Logs (2025-11-15 00:42:59) <<<\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "async def main():\n",
    "    job_config = make_job_config()\n",
    "\n",
    "    try:\n",
    "        # 1. Get job state and create process mesh\n",
    "        job_state = slurm_job.state()\n",
    "        proc_mesh = job_state.mesh0.spawn_procs({\"gpus\": RunParams.gpus_per_node})\n",
    "        \n",
    "        # 2. Configure remote logging behavior\n",
    "        await proc_mesh.logging_option(\n",
    "            stream_to_client=True,\n",
    "            # aggregate_window_sec=None  # Uncomment to disable log batching\n",
    "        )\n",
    "        \n",
    "        # 3. Setup environment for torch.distributed\n",
    "        await setup_env_for_distributed(proc_mesh)\n",
    "        \n",
    "        # 4. Spawn TrainerActor on each GPU\n",
    "        trainer = proc_mesh.spawn(\"trainer_actor\", TrainerActor, job_config)\n",
    "        \n",
    "        # 5. Execute the training job\n",
    "        await trainer.start_training.call()\n",
    "        \n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training workflow failed: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13bf71",
   "metadata": {},
   "source": [
    "### 4. Cleanup the SLURM job\n",
    "Once you're done experimenting, free up the allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c10aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancelled SLURM job 7749\n",
      "slurm.utils 2025-11-15 00:42:59 INFO Job terminated successfully\n"
     ]
    }
   ],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "await cleanup_job(slurm_job)"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "2fa680ca-06ba-41e3-ac40-90b22d77bbc3",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python (monarch)",
   "language": "python",
   "name": "monarch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
