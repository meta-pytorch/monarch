{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c8184767-4307-482c-850f-c19cbe49f16a",
            "metadata": {},
            "source": [
                "# DDP Examples Using Classic SPMD / torch.distributed\n",
                "Let's see if we can run torch's basic [ddp example](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html), but wrapped in Monarch actors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc7ef2eb-e3ad-4caf-b5f7-f6352970bc2c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "import torch\n",
                "import torch.distributed as dist\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "from monarch.proc_mesh import proc_mesh\n",
                "from monarch.actor_mesh import Actor, current_rank, endpoint\n",
                "\n",
                "from torch.nn.parallel import DistributedDataParallel as DDP\n",
                "\n",
                "\n",
                "WORLD_SIZE = 4\n",
                "\n",
                "\n",
                "class ToyModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(ToyModel, self).__init__()\n",
                "        self.net1 = nn.Linear(10, 10)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.net2 = nn.Linear(10, 5)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net2(self.relu(self.net1(x)))\n",
                "\n",
                "\n",
                "class DDPActor(Actor):\n",
                "    \"\"\"This Actor wraps the basic functionality from Torch's DDP example. Conveniently, all of the\n",
                "    methods we need are already laid out for us, so we can just wrap them in the usual Actor endpoint semantic with some light modifications\n",
                "\n",
                "    # copy pasta from https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        self.rank = current_rank().rank\n",
                "\n",
                "    def _rprint(self, msg):\n",
                "        print(f\"{self.rank=} {msg}\")\n",
                "\n",
                "    @endpoint\n",
                "    async def setup(self):\n",
                "        self._rprint(\"Initializing torch distributed\")\n",
                "\n",
                "        # initialize the process group\n",
                "        dist.init_process_group(\"gloo\", rank=self.rank, world_size=WORLD_SIZE)\n",
                "        self._rprint(\"Finished initializing torch distributed\")\n",
                "\n",
                "    @endpoint\n",
                "    async def cleanup(self):\n",
                "        self._rprint(\"Cleaning up torch distributed\")\n",
                "        dist.destroy_process_group()\n",
                "\n",
                "    @endpoint\n",
                "    async def demo_basic(self):\n",
                "        self._rprint(\"Running basic DDP example\")\n",
                "        # setup(rank, world_size)\n",
                "\n",
                "        # create model and move it to GPU with id rank\n",
                "        model = ToyModel().to(self.rank)\n",
                "        ddp_model = DDP(model, device_ids=[self.rank])\n",
                "\n",
                "        loss_fn = nn.MSELoss()\n",
                "        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = ddp_model(torch.randn(20, 10))\n",
                "        labels = torch.randn(20, 5).to(self.rank)\n",
                "        loss_fn(outputs, labels).backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        # cleanup()\n",
                "        print(f\"{self.rank=} Finished running basic DDP example\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cc47e45-cde8-4376-9d47-1d629871a9e1",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Spawn a process mesh\n",
                "local_proc_mesh = await proc_mesh(\n",
                "    gpus=WORLD_SIZE,\n",
                "    env={\n",
                "        \"MASTER_ADDR\": \"localhost\",\n",
                "        \"MASTER_PORT\": \"12355\",\n",
                "    },\n",
                ")\n",
                "# Spawn our actor mesh on top of the process mesh\n",
                "ddp_actor = await local_proc_mesh.spawn(\"ddp_actor\", DDPActor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87a77cb6-57fe-40eb-9f09-bfb51b64b5c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup torch Distributed\n",
                "await ddp_actor.setup.call()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "408a1ef6-6fab-41cc-816e-95c115577a51",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the demo\n",
                "await ddp_actor.demo_basic.call()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3beeb159-8b8f-4ec7-ae5a-59f041726fb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the demo\n",
                "await ddp_actor.cleanup.call()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
