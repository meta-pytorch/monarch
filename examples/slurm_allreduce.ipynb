{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c443b989-5a71-455f-9a59-9963338634ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx.schedulers.slurm_scheduler 2025-08-28 22:04:57 INFO unable to get job info for `monarch-ubuntu` with `squeue` (squeue: error: Invalid job id: monarch-ubuntu\n",
      "), trying `sacct`\n",
      "torchx.schedulers.slurm_scheduler 2025-08-28 22:04:57 INFO unable to get job info for `monarch-ubuntu` with `sacct` (sacct: fatal: Bad job/step specified: monarch-ubuntu\n",
      ")\n",
      "monarch.tools.commands 2025-08-28 22:04:57 INFO no existing RUNNING server `slurm:///monarch-ubuntu` creating new one...\n",
      "torchx.runner.api 2025-08-28 22:04:57 INFO Tracker configurations: {}\n",
      "torchx.runner.api 2025-08-28 22:04:57 INFO Checking for changes in workspace `/home/ubuntu/ahmads/monarch/examples`...\n",
      "torchx.runner.api 2025-08-28 22:04:57 INFO To disable workspaces pass: --workspace=\"\" from CLI or workspace=None programmatically.\n",
      "torchx.runner.api 2025-08-28 22:04:57 INFO Reusing original image `monarch_default_workspace:latest` for role[0]=mesh0. Either a patch was built or no changes to workspace was detected.\n",
      "monarch.tools.commands 2025-08-28 22:04:57 INFO created new `slurm:///336` waiting for it to be ready...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmad: {'requeue': None, 'ntasks-per-node': '1', 'cpus-per-task': '48', 'mem': '186777', 'gpus-per-task': '4', 'ntasks': '1'}\n",
      "Ahmad: {'requeue': None, 'ntasks-per-node': '1', 'cpus-per-task': '48', 'mem': '186777', 'gpus-per-task': '4', 'ntasks': '1'}\n",
      "Waiting for slurm:///336 to be RUNNING (current: PENDING); will check again in 5.0 seconds. Total wait time: 0:00:10.057235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ 2025-08-28 22:05:12 INFO \n",
      "===== Server Info =====\n",
      "{\n",
      "  \"name\": \"336\",\n",
      "  \"server_handle\": \"slurm:///336\",\n",
      "  \"state\": \"RUNNING\",\n",
      "  \"meshes\": {\n",
      "    \"mesh0\": {\n",
      "      \"host_type\": \"__UNSET__\",\n",
      "      \"hosts\": 2,\n",
      "      \"gpus\": -1,\n",
      "      \"hostnames\": [\n",
      "        \"gpu-queue-st-gpu-compute-1\",\n",
      "        \"gpu-queue-st-gpu-compute-2\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "__main__ 2025-08-28 22:05:12 INFO computing world size...\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO no match label `procmesh.monarch.meta.com/name` specified in alloc constraints\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO found a single proc mesh `mesh0` in slurm:///336, will allocate on it\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-1:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO resolved AF_INET address `10.0.2.165:26600` for `gpu-queue-st-gpu-compute-1:26600`\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-2:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO resolved AF_INET address `10.0.2.105:26600` for `gpu-queue-st-gpu-compute-2:26600`\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO initializing alloc on remote allocator addresses: ['tcp!10.0.2.165:26600', 'tcp!10.0.2.105:26600']\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO no match label `procmesh.monarch.meta.com/name` specified in alloc constraints\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO found a single proc mesh `mesh0` in slurm:///336, will allocate on it\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-1:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO resolved AF_INET address `10.0.2.165:26600` for `gpu-queue-st-gpu-compute-1:26600`\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-2:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-28 22:05:12 INFO resolved AF_INET address `10.0.2.105:26600` for `gpu-queue-st-gpu-compute-2:26600`\n",
      "monarch._src.actor.allocator 2025-08-28 22:05:12 INFO initializing alloc on remote allocator addresses: ['tcp!10.0.2.165:26600', 'tcp!10.0.2.105:26600']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New job `slurm:///336` is ready to serve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ 2025-08-28 22:05:19 INFO computed world_sizes:\n",
      "    ----------------------------------------\n",
      "    {\n",
      "  \"rank_0\": 8,\n",
      "  \"rank_1\": 8,\n",
      "  \"rank_2\": 8,\n",
      "  \"rank_3\": 8,\n",
      "  \"rank_4\": 8,\n",
      "  \"rank_5\": 8,\n",
      "  \"rank_6\": 8,\n",
      "  \"rank_7\": 8\n",
      "}\n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-08-28 22:05:18) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m Initializing process group `nccl`:\n",
      "\u001b[33m[8 similar log lines]\u001b[0m   MASTER_ADDR = gpu-queue-st-gpu-compute-1\n",
      "\u001b[33m[8 similar log lines]\u001b[0m   MASTER_PORT = 29500\n",
      "\u001b[33m[8 similar log lines]\u001b[0m   RANK        = 0\n",
      "\u001b[33m[8 similar log lines]\u001b[0m   WORLD_SIZE  = 8\n",
      "\u001b[36m<<< Aggregated Logs (2025-08-28 22:05:19) <<<\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "# @noautodeps\n",
    "# pyre-ignore-all-errors\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import getpass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/ahmads/monarch/examples\")\n",
    "from compute_world_size_actor import TestActor\n",
    "\n",
    "from monarch._rust_bindings.monarch_hyperactor.alloc import AllocConstraints, AllocSpec\n",
    "\n",
    "# from monarch._src.actor.meta.allocator import MastAllocator, MastAllocatorConfig\n",
    "\n",
    "from monarch.actor import ProcMesh\n",
    "from monarch.tools import commands\n",
    "from monarch.tools.components import hyperactor\n",
    "from monarch.tools.config import Config, UnnamedAppDef\n",
    "from monarch._src.actor.allocator import (\n",
    "    RemoteAllocator,\n",
    "    StaticRemoteAllocInitializer,\n",
    "    TorchXRemoteAllocInitializer,\n",
    ")\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from monarch.actor import Actor, current_rank, current_size, endpoint\n",
    "\n",
    "\n",
    "USER = getpass.getuser()\n",
    "HOME = pathlib.Path().home()\n",
    "CWD = os.getcwd()\n",
    "DEACTIVATE = None\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s %(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "FORCE_RESTART = False\n",
    "\n",
    "\n",
    "async def get_appdef(num_hosts):\n",
    "    # similar to Docker image; should contain a conda env in the $img_root/conda/ directory\n",
    "    # when config.workspace is not None, an ephemeral fbpkg version is created\n",
    "    # that conda-packs the currently active local conda env AND the directory specified by workspace\n",
    "    image = \"monarch_default_workspace:latest\"\n",
    "\n",
    "    appdef = hyperactor.host_mesh(\n",
    "        image=image,\n",
    "        # TODO: For some reason gpu.medium doens't work here\n",
    "        meshes=[f\"mesh0:{num_hosts}:aws_g5.12xlarge\"],  # mesh_name:num_hosts:host_type\n",
    "    )\n",
    "    return appdef\n",
    "\n",
    "\n",
    "async def get_server_info(appdef):\n",
    "    jobname = f\"monarch-{USER}\"\n",
    "\n",
    "    # TODO: Register this so we don't have to do this every time\n",
    "    for role in appdef.roles:\n",
    "        role.resource.memMB = 186777\n",
    "\n",
    "    config = Config(\n",
    "        scheduler=\"slurm\",\n",
    "        scheduler_args={\n",
    "            # NOTE: replace with your own values\n",
    "            \"hpcIdentity\": \"pytorch_distributed\",\n",
    "            \"hpcJobOncall\": \"monarch\",\n",
    "            \"hpcClusterUuid\": \"MastProdCluster\",\n",
    "            \"rmAttribution\": \"pytorch4all_clients_approved\",\n",
    "        },\n",
    "        appdef=appdef,\n",
    "        workspace=str(CWD),  # or None to disable building ephemeral,\n",
    "    )\n",
    "\n",
    "    # config.dryrun = True\n",
    "    # o = commands.create(config)\n",
    "    # print(o)\n",
    "    # sys.exit(0)\n",
    "\n",
    "    server_info = await commands.get_or_create(\n",
    "        jobname,\n",
    "        config,\n",
    "        force_restart=FORCE_RESTART,\n",
    "    )\n",
    "    return server_info\n",
    "\n",
    "\n",
    "async def create_proc_mesh(num_hosts, appdef, server_info):\n",
    "    # TODO: why is gpus equal to -1 in server_info?\n",
    "\n",
    "    num_gpus_per_host = appdef.roles[0].resource.gpu\n",
    "\n",
    "    logger.info(\n",
    "        \"\\n===== Server Info =====\\n%s\",\n",
    "        json.dumps(server_info.to_json(), indent=2),\n",
    "    )\n",
    "\n",
    "    mesh_dimensions = {\n",
    "        \"host\": server_info.get_mesh_spec(\"mesh0\").num_hosts,\n",
    "        \"gpu\": server_info.get_mesh_spec(\"mesh0\").gpus,\n",
    "    }\n",
    "\n",
    "    allocator = RemoteAllocator(\n",
    "        world_id=\"foo\",\n",
    "        initializer=TorchXRemoteAllocInitializer(server_info.server_handle),\n",
    "    )\n",
    "    alloc = await allocator.allocate(\n",
    "        AllocSpec(AllocConstraints(), hosts=num_hosts, gpus=num_gpus_per_host)\n",
    "    )\n",
    "\n",
    "    proc_mesh = await ProcMesh.from_alloc(alloc)\n",
    "    return proc_mesh\n",
    "\n",
    "\n",
    "async def main():\n",
    "    num_hosts = 2\n",
    "    appdef = await get_appdef(num_hosts)\n",
    "    server_info = await get_server_info(appdef)\n",
    "\n",
    "    try:\n",
    "        proc_mesh = await create_proc_mesh(num_hosts, appdef, server_info)\n",
    "        actor = await proc_mesh.spawn(\"compute_world_size_actor\", TestActor)\n",
    "\n",
    "        logger.info(\"computing world size...\")\n",
    "        # this is redundant but is here for example sake\n",
    "        mesh_name = server_info.get_mesh_spec(\"mesh0\").name\n",
    "        values = await actor.compute_world_size.call(\n",
    "            master_addr=server_info.host0(mesh_name),\n",
    "            master_port=29500,\n",
    "        )\n",
    "\n",
    "        values_by_rank = {f\"rank_{p.rank}\": v for p, v in list(values.flatten(\"rank\"))}\n",
    "\n",
    "        logger.info(\n",
    "            f\"\"\"computed world_sizes:\n",
    "    {'-'*40}\n",
    "    {json.dumps(values_by_rank, indent=2)}\n",
    "    {'-'*40}\"\"\"\n",
    "        )\n",
    "    finally:\n",
    "        commands.kill(f\"slurm:///{server_info.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cloudpickle.register_pickle_by_value(sys.modules[TestActor.__module__])\n",
    "\n",
    "    # asyncio.run(main())\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fcfc7-3561-43bd-9945-278fb488e0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
