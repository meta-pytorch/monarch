{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "private_master_host_ip_address = 10.192.10.43\n",
      "public_master_host_ip_address = 34.201.107.243\n",
      "public_master_host_ip_address = 34.201.107.243\n"
     ]
    }
   ],
   "source": [
    "from utils.master_node import MasterNodeServer\n",
    "private_master_host_ip_address = MasterNodeServer.get_master_ip()\n",
    "public_master_host_ip_address = MasterNodeServer.get_master_public_ip_curl()\n",
    "public_master_host_ip_address_services = MasterNodeServer.get_master_public_ip()\n",
    "print(f\"private_master_host_ip_address = {private_master_host_ip_address}\")\n",
    "print(f\"public_master_host_ip_address = {public_master_host_ip_address}\")\n",
    "print(f\"public_master_host_ip_address = {public_master_host_ip_address_services}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "NUM_NODES = 2\n",
    "NUM_CPUS = 2\n",
    "NUM_GPUS = 8\n",
    "NUM_PROCS = NUM_NODES * NUM_GPUS\n",
    "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
    "USER = \"meta-ai\"  # Replace with your username\n",
    "MONARCH_DEFAULT_PORT = 26600 # Monarch default port\n",
    "HTTP_SERVER_PORT = MONARCH_DEFAULT_PORT # 8080 # HTTP Server PORT for IP registration\n",
    "MMT_JOB_NAME = f\"Monarch-v1-Titan-{NUM_NODES}_nodes-port_override\"\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    # Install the MMT plugin befor running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with CPUs\n",
    "    # machine_type = getattr(Machine, f\"CPU_X_{NUM_CPUS}\")\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{NUM_GPUS}\")\n",
    "\n",
    "     # Machine with L4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"L4_X_{NUM_GPUS}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
    "\n",
    "    job = MMT.run(\n",
    "        command=\"process_allocator\",\n",
    "        # command=f\"tail -f /dev/null\",\n",
    "        name=MMT_JOB_NAME,\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible # TODO: Should make this one dynamic\n",
    "            \"MONARCH_FILE_LOG\": \"debug\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\": \"26601-26610\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\": \"true\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    # Monitor job status\n",
    "    return job, studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching MMT job with 2 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/Monarch-v1-Titan-2_nodes-port_override-437zt?app_id=mmt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: Monarch-v1-Titan-2_nodes-port_override-437zt\n",
      "Job status: Pending\n",
      "Job launched. You can monitor it using: job.status\n",
      "To stop the job: job.stop()\n",
      "To clean up: studio.stop()\n"
     ]
    }
   ],
   "source": [
    "# Launch the job\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip_addresses_list=['3.150.40.243', '18.189.125.53']\n",
      "ip_addresses_set={'18.189.125.53', '3.150.40.243'}\n",
      "IP addresses are available: True\n"
     ]
    }
   ],
   "source": [
    "ip_addresses_list = [machine.public_ip for machine in job.machines]\n",
    "ip_addresses_set = set(ip_addresses_list)\n",
    "print(f\"{ip_addresses_list=}\")\n",
    "print(f\"{ip_addresses_set=}\")\n",
    "ips_available = not ip_addresses_set == {''}\n",
    "print(f\"IP addresses are available: {ips_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tcp!18.189.125.53:26600', 'tcp!3.150.40.243:26600']\n"
     ]
    }
   ],
   "source": [
    "if ips_available:\n",
    "    tcp_addresses = [f\"tcp!{ip}:{MONARCH_DEFAULT_PORT}\" for ip in ip_addresses_set]\n",
    "    print(tcp_addresses)\n",
    "else:\n",
    "    raise ValueError(\"IPs are not available yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<monarch._src.actor.allocator.RemoteAllocator object at 0x79c87419ad50>\n",
      "tcp!34.201.107.243:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from monarch._src.actor.allocator import RemoteAllocator, StaticRemoteAllocInitializer\n",
    "# from monarch._rust_bindings.monarch_hyperactor.alloc import AllocConstraints, AllocSpec\n",
    "# from monarch.actor import ProcMesh\n",
    "# tcp_addresses = ['tcp!3.21.117.93:26600', 'tcp!18.220.66.230:26600']\n",
    "\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\"] = \"26600-26610\"\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"] = f\"tcp!{public_master_host_ip_address}:0\"\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\"] = \"true\"\n",
    "os.environ[\"MONARCH_HOST_MESH_V1_REMOVE_ME_BEFORE_RELEASE\"] = \"1\"\n",
    "\n",
    "allocator = RemoteAllocator(\n",
    "        world_id=\"foo\",\n",
    "        initializer=StaticRemoteAllocInitializer(*tcp_addresses),\n",
    "    )\n",
    "\n",
    "print(allocator)\n",
    "print(os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.actor import HostMesh\n",
    "from monarch._rust_bindings.monarch_hyperactor.shape import Extent\n",
    "\n",
    "host_mesh = HostMesh.allocate_nonblocking(\n",
    "        \"hostmeshtest\",\n",
    "        extent=Extent([\"hosts\", \"procs\"], [NUM_NODES, NUM_PROCS]),\n",
    "        allocator=allocator,\n",
    "    )\n",
    "proc_mesh = host_mesh.spawn_procs({\"gpus\": NUM_GPUS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch-alisol-hosts2-gpus8\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert (\n",
    "                    # config.checkpoint.enable_checkpoint\n",
    "                    config.checkpoint.enable\n",
    "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0, )\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.xpu import stream\n",
    "from torchtitan.config import JobConfig\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    await setup_env_for_distributed(proc_mesh, )\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    # trainer_actor = await proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "    trainer_actor = proc_mesh.spawn_procs(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[titan] 2025-10-15 03:18:27,751 - root - WARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.\n"
     ]
    }
   ],
   "source": [
    "from torchtitan.config import ConfigManager\n",
    "from torchtitan.tools.logging import init_logger\n",
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "        \"--model.tokenizer-path\",\n",
    "        # f\"{FUSE_DST}/Llama-3.1-8B\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "        \"--training.steps\",\n",
    "        \"25\",\n",
    "        \"--training.dataset_path\",\n",
    "        # f\"{FUSE_DST}/c4\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "        \"--job.dump_folder\",\n",
    "        # f\"{FUSE_DST}/outputs/\" + job_name,\n",
    "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "        \"--training.seq_len\",\n",
    "        \"1024\",\n",
    "        # \"8192\",\n",
    "    ]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.job import SlurmJob, JobTrait"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
