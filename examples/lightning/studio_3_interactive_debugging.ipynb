{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Studio 3: Interactive Debugging for Distributed Training\n\nWelcome to Studio 3! In this notebook, you'll master **interactive debugging** techniques for distributed systems using Monarch.\n\n## The Challenge\n\nDebugging distributed training is notoriously difficult:\n- Issues may only appear on specific ranks or nodes\n- Traditional debuggers don't work across multiple processes\n- Environment differences between nodes are hard to inspect\n- Logs from 128+ processes are overwhelming\n\n## Monarch's Solution\n\nMonarch provides powerful debugging capabilities:\n1. **Interactive breakpoints** - Use `pdb` with distributed actors\n2. **Selective debugging** - Attach to specific ranks\n3. **Environment inspection** - Query env vars across all nodes\n4. **Monarch debug CLI** - Unified interface for distributed debugging\n\n## What You'll Learn\n\n### Environment Variable Management\n- Inspect environment variables across nodes\n- Set and modify env vars remotely\n- Query variables by prefix (CUDA, NCCL, etc.)\n\n### Interactive Debugging with Breakpoints\n- Add breakpoints to actor methods\n- Use `monarch debug` CLI\n- Attach to specific ranks for interactive debugging\n- Send debugger commands to multiple ranks\n\n## Prerequisites\n\n**Recommended:** Complete [Studio 1: Getting Started](./studio_1_getting_started.ipynb) and [Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb) first!\n\nYou should have:\n- A running multi-node Lightning job\n- An initialized Monarch process mesh\n- Understanding of Monarch actors and endpoints\n\n**New to Monarch?** Start with [Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb) to learn about Actors, Endpoints, and Process Meshes!\n\n## Lightning Studios Series\n\nThis is **Studio 3** of the series:\n\n- **[Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb)** - Learn Monarch fundamentals\n- **[Studio 1: Getting Started](./studio_1_getting_started.ipynb)** - Multi-node training\n- **[Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)** - Hot-reload configs\n- **Studio 3: Interactive Debugging** - Debug distributed systems (YOU ARE HERE)\n\nLet's dive in!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup (If Starting Fresh)\n",
    "\n",
    "If you're continuing from Studio 1 or 2, **skip this section**. Otherwise, run these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if starting fresh\n",
    "from lightning_sdk import Machine, MMT, Studio\n",
    "import os\n",
    "\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"\n",
    "USER = \"your-username\"\n",
    "\n",
    "os.environ[\"MONARCH_V0_WORKAROUND_DO_NOT_USE\"] = \"1\"\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "\n",
    "# Launch job and setup proc_mesh (see Studio 1 for details)\n",
    "# job, studio = launch_mmt_job(...)\n",
    "# proc_mesh = setup_proc_mesh_from_job(job, NUM_NODES, NUM_GPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Environment Variable Management\n",
    "\n",
    "Let's start by creating an actor that can inspect and manage environment variables across all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment Variable Actor\n",
    "\n",
    "This actor provides methods to get, set, and list environment variables on remote nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.actor import Actor, endpoint, current_rank\n",
    "import os\n",
    "import socket\n",
    "\n",
    "\n",
    "class EnvVarActor(Actor):\n",
    "    \"\"\"Actor for managing environment variables on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def get_env(self, var_name: str) -> dict:\n",
    "        \"\"\"Get an environment variable value from the remote node.\"\"\"\n",
    "        value = os.environ.get(var_name)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": value\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def set_env(self, var_name: str, var_value: str) -> dict:\n",
    "        \"\"\"Set an environment variable on the remote node.\"\"\"\n",
    "        os.environ[var_name] = var_value\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": var_value,\n",
    "            \"status\": \"set\"\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def list_env_vars(self, prefix: str = \"\") -> dict:\n",
    "        \"\"\"List all environment variables matching a prefix.\"\"\"\n",
    "        matching_vars = {k: v for k, v in os.environ.items() if k.startswith(prefix)}\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"matching_vars\": matching_vars,\n",
    "            \"count\": len(matching_vars)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn Environment Variable Actor\n",
    "\n",
    "Spawn the actor across all nodes in the process mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the environment variable actor across all nodes\n",
    "env_actor = proc_mesh.spawn(\"env_actor\", EnvVarActor)\n",
    "print(\"✓ EnvVarActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Environment Variables\n",
    "\n",
    "Let's inspect CUDA-related environment variables across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CUDA_VISIBLE_DEVICES from all nodes\n",
    "results = await env_actor.get_env.call(\"CUDA_VISIBLE_DEVICES\")\n",
    "\n",
    "print(\"\\nCUDA_VISIBLE_DEVICES on all nodes:\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "# Show unique values by node\n",
    "seen_nodes = set()\n",
    "for result in results:\n",
    "    if len(result) > 1:\n",
    "        rank = result[1].get('rank', '?')\n",
    "        hostname = result[1].get('hostname', '?')\n",
    "        value = result[1].get('value', '?')\n",
    "    else:\n",
    "        rank = result.get('rank', '?')\n",
    "        hostname = result.get('hostname', '?')\n",
    "        value = result.get('value', '?')\n",
    "    \n",
    "    if hostname not in seen_nodes:\n",
    "        print(f\"  Node {hostname} (Rank {rank}): {value}\")\n",
    "        seen_nodes.add(hostname)\n",
    "\n",
    "print(f\"{'-'*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Custom Environment Variables\n",
    "\n",
    "You can set environment variables remotely for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom environment variable on all nodes\n",
    "print(\"Setting CUSTOM_DEBUG_VAR on all nodes...\")\n",
    "set_results = await env_actor.set_env.call(\"CUSTOM_DEBUG_VAR\", \"debug_enabled\")\n",
    "\n",
    "print(f\"\\n✓ Set CUSTOM_DEBUG_VAR on {len(set_results)} ranks\")\n",
    "\n",
    "# Verify the variable was set\n",
    "verify_results = await env_actor.get_env.call(\"CUSTOM_DEBUG_VAR\")\n",
    "print(f\"\\nVerification (first 3 ranks):\")\n",
    "for i, result in enumerate(verify_results[:3]):\n",
    "    if len(result) > 1:\n",
    "        rank = result[1]['rank']\n",
    "        value = result[1]['value']\n",
    "    else:\n",
    "        rank = result['rank']\n",
    "        value = result['value']\n",
    "    print(f\"  Rank {rank}: CUSTOM_DEBUG_VAR = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Variables by Prefix\n",
    "\n",
    "Query all environment variables matching a specific prefix - useful for debugging CUDA, NCCL, or PyTorch settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CUDA-related environment variables\n",
    "list_results = await env_actor.list_env_vars.call(\"CUDA\")\n",
    "\n",
    "print(\"\\nCUDA-related environment variables (Rank 0):\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "if list_results[0]:\n",
    "    result = list_results[0][1] if len(list_results[0]) > 1 else list_results[0]\n",
    "    matching_vars = result.get('matching_vars', {})\n",
    "    \n",
    "    if matching_vars:\n",
    "        for var_name, var_value in matching_vars.items():\n",
    "            # Truncate long values\n",
    "            display_value = var_value if len(var_value) < 60 else var_value[:57] + \"...\"\n",
    "            print(f\"  {var_name} = {display_value}\")\n",
    "    else:\n",
    "        print(\"  No CUDA variables found\")\n",
    "\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "# Try other prefixes\n",
    "print(\"\\n💡 Tip: Try querying other prefixes like:\")\n",
    "print(\"  • 'NCCL' - NCCL communication settings\")\n",
    "print(\"  • 'TORCH' - PyTorch settings\")\n",
    "print(\"  • 'MONARCH' - Monarch-specific configs\")\n",
    "print(\"  • 'MASTER' - Distributed training master node info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Interactive Debugging with Breakpoints\n",
    "\n",
    "Now let's explore Monarch's most powerful debugging feature: **interactive breakpoints** in distributed actors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Monarch Debugging Works\n",
    "\n",
    "### The Workflow\n",
    "\n",
    "1. **Add `breakpoint()`** to your actor methods\n",
    "2. **Run your code** - execution pauses when breakpoint is hit\n",
    "3. **Open a terminal** and run `monarch debug`\n",
    "4. **Use debugger commands**:\n",
    "   - `list` - Show all active breakpoints\n",
    "   - `attach <actor> <rank>` - Attach to a specific rank\n",
    "   - Standard pdb commands: `n`, `s`, `p`, `l`, `c`\n",
    "   - `cast <actor> ranks(<ranks>) <cmd>` - Send commands to multiple ranks\n",
    "   - `continue` - Resume all paused processes\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Debug specific ranks (e.g., only rank 0 or only GPU 3)\n",
    "- Inspect local variables and actor state\n",
    "- Step through code interactively\n",
    "- Send commands to multiple ranks simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Debug Trainer Actor\n",
    "\n",
    "Let's create a simplified trainer with strategic breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "from torchtitan.train import Trainer\n",
    "from torchtitan.tools.logging import logger\n",
    "\n",
    "\n",
    "class DebugTrainerActor(Actor):\n",
    "    \"\"\"TorchTitan Trainer Actor with debugging breakpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "        self.trainer: Optional[Trainer] = None\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"[Rank {self.rank}] {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        self._rprint(f\"Initializing debug actor: {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "        # Breakpoint 1: After initialization (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(\"🔴 Breakpoint 1: Initialization complete\")\n",
    "            breakpoint()  # Debug: Inspect actor initialization state\n",
    "\n",
    "    @endpoint\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Setup the trainer with a breakpoint to inspect configuration.\"\"\"\n",
    "        logger.info(f\"Setting up trainer on rank {self.rank}\")\n",
    "        config = self.job_config\n",
    "\n",
    "        # Breakpoint 2: Before trainer creation (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(\"🔴 Breakpoint 2: About to create trainer\")\n",
    "            self._rprint(f\"Config: batch_size={getattr(config.training, 'batch_size', 'N/A')}\")\n",
    "            breakpoint()  # Debug: Inspect job config before trainer creation\n",
    "\n",
    "        self.trainer = Trainer(config)\n",
    "        self._rprint(\"Trainer setup complete\")\n",
    "\n",
    "    @endpoint\n",
    "    def train_step(self, num_steps: int = 5):\n",
    "        \"\"\"Run a few training steps with breakpoints.\"\"\"\n",
    "        if not self.trainer:\n",
    "            raise RuntimeError(\"Trainer not initialized. Call setup_trainer first.\")\n",
    "\n",
    "        logger.info(f\"Starting training for {num_steps} steps on rank {self.rank}\")\n",
    "\n",
    "        # Breakpoint 3: Before training starts (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(\"🔴 Breakpoint 3: About to start training\")\n",
    "            breakpoint()  # Debug: Inspect trainer state before training\n",
    "\n",
    "        # Simulate training steps\n",
    "        for step in range(num_steps):\n",
    "            self.step_count += 1\n",
    "            \n",
    "            # Breakpoint 4: Mid-training on rank 0 at step 2\n",
    "            if step == 2 and self.rank == 0:\n",
    "                self._rprint(f\"🔴 Breakpoint 4: Mid-training (step {self.step_count})\")\n",
    "                breakpoint()  # Debug: Inspect mid-training state\n",
    "\n",
    "            self._rprint(f\"Processing step {step + 1}/{num_steps}\")\n",
    "\n",
    "        self._rprint(f\"Completed {num_steps} training steps\")\n",
    "\n",
    "    @endpoint\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources.\"\"\"\n",
    "        logger.info(f\"Cleaning up trainer on rank {self.rank}\")\n",
    "\n",
    "        if self.trainer:\n",
    "            self.trainer.close()\n",
    "\n",
    "        if torch.distributed.is_initialized():\n",
    "            torch.distributed.destroy_process_group()\n",
    "\n",
    "        self._rprint(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn Debug Trainer\n",
    "\n",
    "Spawn the debug trainer actor. When you run the cells below, execution will pause at breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager\n",
    "\n",
    "# Parse config (using simple defaults for debugging)\n",
    "config_manager = ConfigManager()\n",
    "manual_args = [\n",
    "    \"--job.config_file\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\",\n",
    "    \"--training.steps\", \"5\",\n",
    "]\n",
    "debug_config = config_manager.parse_args(manual_args)\n",
    "\n",
    "# Spawn the debug trainer actor\n",
    "debug_trainer = proc_mesh.spawn(\"debug_trainer\", DebugTrainerActor, debug_config)\n",
    "print(\"✓ Debug trainer actor spawned across all nodes\")\n",
    "print(\"\\n⚠️ When breakpoints are hit, execution will pause.\")\n",
    "print(\"📍 Open a separate terminal and run: monarch debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debug Session\n",
    "\n",
    "Now let's run the training methods. When breakpoints are hit:\n",
    "\n",
    "### In This Notebook\n",
    "- Execution will pause\n",
    "- You'll see `🔴 Breakpoint X: ...` messages\n",
    "\n",
    "### In a Separate Terminal\n",
    "1. Run: `monarch debug`\n",
    "2. Use `list` to see all active breakpoints\n",
    "3. Use `attach debug_trainer 0` to attach to rank 0\n",
    "4. Use standard pdb commands or `continue` to resume\n",
    "\n",
    "**Note:** For this demo, we'll skip the interactive debugging. In practice, you'd have two terminals open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize actors (will hit breakpoint 1)\n",
    "print(\"📍 Step 1: Initializing actors...\")\n",
    "print(\"   (Breakpoint 1 will trigger on rank 0)\\n\")\n",
    "\n",
    "# In a real scenario, this would pause at the breakpoint\n",
    "# await debug_trainer.init.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer (will hit breakpoint 2)\n",
    "print(\"📍 Step 2: Setting up trainer...\")\n",
    "print(\"   (Breakpoint 2 will trigger on rank 0)\\n\")\n",
    "\n",
    "# await debug_trainer.setup_trainer.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training steps (will hit breakpoints 3 and 4)\n",
    "print(\"📍 Step 3: Running training steps...\")\n",
    "print(\"   (Breakpoints 3 and 4 will trigger on rank 0)\\n\")\n",
    "\n",
    "# await debug_trainer.train_step.call(num_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monarch Debug CLI Commands\n",
    "\n",
    "Here's a quick reference for the `monarch debug` CLI:\n",
    "\n",
    "### Listing Breakpoints\n",
    "```bash\n",
    "monarch_dbg> list\n",
    "# Shows all active breakpoints across ranks\n",
    "# Example output:\n",
    "#   debug_trainer (rank 0): /path/to/file.py:42\n",
    "#   debug_trainer (rank 0): /path/to/file.py:58\n",
    "```\n",
    "\n",
    "### Attaching to a Rank\n",
    "```bash\n",
    "monarch_dbg> attach debug_trainer 0\n",
    "# Enters interactive pdb session for rank 0\n",
    "\n",
    "(Pdb) n              # Next line\n",
    "(Pdb) s              # Step into function\n",
    "(Pdb) p self.rank    # Print variable\n",
    "(Pdb) l              # List source code\n",
    "(Pdb) pp self.job_config  # Pretty-print object\n",
    "(Pdb) c              # Continue execution\n",
    "```\n",
    "\n",
    "### Casting Commands to Multiple Ranks\n",
    "```bash\n",
    "# Send \"next\" command to ranks 0 and 1\n",
    "monarch_dbg> cast debug_trainer ranks(0,1) n\n",
    "\n",
    "# Send \"continue\" to ranks 0 through 7\n",
    "monarch_dbg> cast debug_trainer ranks(0:8) c\n",
    "\n",
    "# Print a variable on multiple ranks\n",
    "monarch_dbg> cast debug_trainer ranks(0,1,2,3) p self.step_count\n",
    "```\n",
    "\n",
    "### Continuing All\n",
    "```bash\n",
    "monarch_dbg> continue\n",
    "# Resumes execution on all paused ranks\n",
    "```\n",
    "\n",
    "### Getting Help\n",
    "```bash\n",
    "monarch_dbg> help\n",
    "# Shows all available commands\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Debugging Scenarios\n",
    "\n",
    "### Scenario 1: Rank-Specific Bug\n",
    "```python\n",
    "# Problem: Training fails on rank 5 but works on other ranks\n",
    "\n",
    "@endpoint\n",
    "def train(self):\n",
    "    if self.rank == 5:\n",
    "        breakpoint()  # Only pause rank 5\n",
    "    # ... training code\n",
    "```\n",
    "\n",
    "Then in terminal:\n",
    "```bash\n",
    "monarch debug\n",
    "monarch_dbg> attach trainer_actor 5\n",
    "(Pdb) p self.data_batch  # Inspect what's different on rank 5\n",
    "```\n",
    "\n",
    "### Scenario 2: Collective Operation Hang\n",
    "```python\n",
    "# Problem: All-reduce hangs, need to check all ranks\n",
    "\n",
    "@endpoint\n",
    "def sync_gradients(self):\n",
    "    breakpoint()  # Pause all ranks before all-reduce\n",
    "    torch.distributed.all_reduce(self.gradients)\n",
    "```\n",
    "\n",
    "Then:\n",
    "```bash\n",
    "monarch_dbg> list  # Check which ranks hit the breakpoint\n",
    "monarch_dbg> cast trainer_actor ranks(0:8) p self.gradients.shape\n",
    "# Verify all ranks have same shape\n",
    "```\n",
    "\n",
    "### Scenario 3: Environment Mismatch\n",
    "```python\n",
    "# Problem: Different NCCL settings causing issues\n",
    "\n",
    "# Use EnvVarActor to inspect\n",
    "results = await env_actor.list_env_vars.call(\"NCCL\")\n",
    "# Compare NCCL settings across all ranks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 Congratulations! 🎉\n",
    "\n",
    "You've mastered **interactive debugging** for distributed training with Monarch!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### Environment Variable Management\n",
    "- ✓ Query env vars across all nodes\n",
    "- ✓ Set and modify env vars remotely\n",
    "- ✓ List variables by prefix (CUDA, NCCL, etc.)\n",
    "\n",
    "### Interactive Debugging\n",
    "- ✓ Add breakpoints to distributed actors\n",
    "- ✓ Use `monarch debug` CLI\n",
    "- ✓ Attach to specific ranks\n",
    "- ✓ Send commands to multiple ranks\n",
    "- ✓ Common debugging scenarios\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Debug like local code** - Use familiar pdb commands in distributed settings\n",
    "- **Selective debugging** - Focus on problematic ranks without noise from others\n",
    "- **Environment inspection** - Quickly identify configuration mismatches\n",
    "- **No more print debugging** - Interactive inspection is much more powerful\n",
    "\n",
    "## The Complete Monarch Workflow\n",
    "\n",
    "You've now learned the three pillars of efficient distributed development:\n",
    "\n",
    "1. **Studio 1: Getting Started** - Launch multi-node training\n",
    "2. **Studio 2: Workspace Sync** - Hot-reload configs and code\n",
    "3. **Studio 3: Interactive Debugging** - Debug efficiently (YOU ARE HERE!)\n",
    "\n",
    "Together, these enable:\n",
    "- **10x faster iteration** (no job restarts)\n",
    "- **Easier debugging** (interactive breakpoints)\n",
    "- **Better observability** (env var inspection, log aggregation)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Put It Into Practice\n",
    "Try debugging your own training code:\n",
    "1. Add strategic breakpoints\n",
    "2. Run `monarch debug` when they're hit\n",
    "3. Inspect state and identify issues\n",
    "\n",
    "### Explore More\n",
    "- Review [Studio 1: Getting Started](./studio_1_getting_started.ipynb)\n",
    "- Review [Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)\n",
    "- Check out the [Monarch documentation](https://github.com/meta-pytorch/monarch)\n",
    "\n",
    "---\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "### Debugging Best Practices\n",
    "1. **Use conditional breakpoints** - Only pause specific ranks\n",
    "2. **Check env vars first** - Many issues are configuration mismatches\n",
    "3. **Use `cast` for comparison** - Check variables across multiple ranks\n",
    "4. **Don't forget `continue`** - Resume execution when done debugging\n",
    "\n",
    "### Performance Tip\n",
    "Remove or comment out `breakpoint()` calls for production runs - they have minimal overhead when not triggered, but it's cleaner to remove them.\n",
    "\n",
    "Happy debugging! 🐛🔧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}