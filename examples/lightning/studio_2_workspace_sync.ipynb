{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Studio 2: Hot-Reloading with Workspace Synchronization\n\nWelcome to Studio 2! In this notebook, you'll learn one of Monarch's most powerful features: **workspace synchronization**.\n\n## The Problem\n\nIn traditional distributed training:\n1. You launch a multi-node job (takes 5-10 minutes)\n2. You realize you need to change a config value (e.g., learning rate)\n3. You have to **stop everything** and restart (another 5-10 minutes)\n4. Rinse and repeat...\n\nThis is incredibly frustrating and wastes valuable time and compute resources!\n\n## The Solution: Workspace Sync\n\nWith Monarch's `proc_mesh.sync_workspace()`:\n1. Launch your multi-node job once\n2. Edit configs or code **locally**\n3. Run `sync_workspace()` to propagate changes to all remote nodes\n4. Re-run training with updated configs - **no restart needed!**\n\n## What You'll Learn\n\n- How workspace synchronization works\n- Creating and modifying training configs locally\n- Syncing changes to remote worker nodes\n- Verifying synchronization across the cluster\n- Practical hot-reload workflows\n\n## Prerequisites\n\n**Required:** Complete [Studio 1: Getting Started](./studio_1_getting_started.ipynb) first!\n\nYou should have:\n- A running multi-node Lightning job\n- An initialized Monarch process mesh\n- Basic understanding of Monarch actors\n\n**New to Monarch?** Start with [Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb) to learn the fundamentals!\n\n## Lightning Studios Series\n\nThis is **Studio 2** of the series:\n\n- **[Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb)** - Learn Monarch fundamentals\n- **[Studio 1: Getting Started](./studio_1_getting_started.ipynb)** - Multi-node training\n- **Studio 2: Workspace Sync** - Hot-reload configs (YOU ARE HERE)\n- **[Studio 3: Interactive Debugging](./studio_3_interactive_debugging.ipynb)** - Debug distributed systems\n\n## Quick Recap from Studio 1\n\nIf you completed Studio 1, you should have:\n- `job` - Your Lightning MMT job\n- `proc_mesh` - Your Monarch process mesh\n- `NUM_NODES` and `NUM_GPUS` configured\n\nIf you need to restart, run the setup cells from Studio 1 first.\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup (If Starting Fresh)\n",
    "\n",
    "If you're continuing from Studio 1, **skip this section**. If you're starting fresh, run these cells to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if starting fresh (not continuing from Studio 1)\n",
    "from lightning_sdk import Machine, MMT, Studio\n",
    "import os\n",
    "\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"\n",
    "USER = \"your-username\"\n",
    "MMT_JOB_NAME = f\"Monarch-MMT-{NUM_NODES}-nodes\"\n",
    "REMOTE_ALLOWED_PORT_RANGE = \"26601..26611\"\n",
    "\n",
    "os.environ[\"MONARCH_V0_WORKAROUND_DO_NOT_USE\"] = \"1\"\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "\n",
    "# Launch job (see Studio 1 for full details)\n",
    "# job, studio = launch_mmt_job(...)\n",
    "# proc_mesh = setup_proc_mesh_from_job(job, NUM_NODES, NUM_GPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workspace Synchronization Workflow\n",
    "\n",
    "Let's dive into workspace sync with a practical example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define File Checker Actor\n",
    "\n",
    "First, we'll create an actor that can read and verify file contents on remote nodes. This helps us confirm that files are properly synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.actor import Actor, endpoint, current_rank\n",
    "import os\n",
    "import socket\n",
    "\n",
    "\n",
    "class FileCheckerActor(Actor):\n",
    "    \"\"\"Actor to read and verify file contents on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def read_file(self, file_path: str) -> dict:\n",
    "        \"\"\"Read a file and return its contents.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"exists\": True,\n",
    "                \"size\": len(content)\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": \"File not found\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    @endpoint\n",
    "    def file_exists(self, file_path: str) -> dict:\n",
    "        \"\"\"Check if a file exists on the remote node.\"\"\"\n",
    "        exists = os.path.exists(file_path)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"file_path\": file_path,\n",
    "            \"exists\": exists\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn File Checker Actor\n",
    "\n",
    "Spawn the file checker actor across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the file checker actor\n",
    "file_checker = proc_mesh.spawn(\"file_checker\", FileCheckerActor)\n",
    "print(\"FileCheckerActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Configuration File\n",
    "\n",
    "Let's create a training configuration file locally. This simulates a common workflow where you want to tweak hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local workspace directory for our custom config\n",
    "local_workspace = \"/teamspace/studios/this_studio/monarch_sync_example\"\n",
    "os.makedirs(local_workspace, exist_ok=True)\n",
    "\n",
    "# Create a custom training configuration file\n",
    "config_file_name = \"custom_training_config.toml\"\n",
    "local_config_path = os.path.join(local_workspace, config_file_name)\n",
    "\n",
    "# Write initial configuration\n",
    "initial_config = \"\"\"# TorchTitan Custom Training Configuration\n",
    "# Version 1.0 - Initial configuration\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "max_steps = 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\"\n",
    "\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(initial_config)\n",
    "\n",
    "print(f\"✓ Created local config file: {local_config_path}\")\n",
    "print(f\"\\nInitial configuration:\\n{'-'*50}\")\n",
    "print(initial_config)\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workspace and Perform Initial Sync\n",
    "\n",
    "Now we'll create a Monarch `Workspace` object and sync our local directory to all remote nodes.\n",
    "\n",
    "**This is the magic step!** 🪄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.tools.config.workspace import Workspace\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Workspace object pointing to our local directory\n",
    "workspace = Workspace(dirs=[Path(local_workspace)])\n",
    "\n",
    "print(f\"Workspace configured: {workspace.dirs}\")\n",
    "print(f\"\\n🔄 Syncing workspace to {NUM_NODES * NUM_GPUS} remote processes...\")\n",
    "\n",
    "# Perform initial sync\n",
    "await proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n",
    "\n",
    "print(\"\\n✅ Initial workspace sync completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify File on Remote Nodes\n",
    "\n",
    "Let's verify that our config file was successfully synced to all remote worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the remote file path (files are synced to WORKSPACE_DIR)\n",
    "remote_workspace_root = os.environ.get(\"WORKSPACE_DIR\", \"/workspace\")\n",
    "remote_config_path = os.path.join(remote_workspace_root, \"monarch_sync_example\", config_file_name)\n",
    "\n",
    "print(f\"Checking file on remote nodes: {remote_config_path}\\n\")\n",
    "\n",
    "# Check file existence on all nodes (just check first rank of each node)\n",
    "exists_results = await file_checker.file_exists.call(remote_config_path)\n",
    "\n",
    "# Group by hostname to show node-level status\n",
    "nodes_checked = set()\n",
    "for result in exists_results:\n",
    "    hostname = result['hostname']\n",
    "    if hostname not in nodes_checked:\n",
    "        status = \"✓ EXISTS\" if result['exists'] else \"✗ NOT FOUND\"\n",
    "        print(f\"  Node {hostname}: {status}\")\n",
    "        nodes_checked.add(hostname)\n",
    "\n",
    "# Read file content from rank 0 to verify\n",
    "print(f\"\\n📄 Reading config from rank 0:\")\n",
    "print(f\"{'-'*50}\")\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    print(read_results[0]['content'])\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hot-Reload: Modify and Re-Sync\n",
    "\n",
    "Now comes the powerful part! Let's modify our config locally and sync it again - **without restarting anything**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Local Configuration\n",
    "\n",
    "Let's say we want to:\n",
    "- Decrease the learning rate (0.001 → 0.0005)\n",
    "- Increase max steps (100 → 200)\n",
    "- Change sequence length (1024 → 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the configuration\n",
    "updated_config = \"\"\"# TorchTitan Custom Training Configuration\n",
    "# Version 2.0 - Updated after initial run\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005  # ← CHANGED: Reduced from 0.001\n",
    "max_steps = 200          # ← CHANGED: Increased from 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 2048           # ← CHANGED: Increased from 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\"\n",
    "\n",
    "# Write updated config locally\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(updated_config)\n",
    "\n",
    "print(f\"✓ Updated local config file: {local_config_path}\")\n",
    "print(f\"\\nUpdated configuration:\\n{'-'*50}\")\n",
    "print(updated_config)\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Sync to Remote Nodes\n",
    "\n",
    "Now sync the changes to all remote nodes. This is instant - no job restart required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🔄 Re-syncing updated workspace to remote nodes...\")\n",
    "\n",
    "# Sync again - Monarch only transfers what changed!\n",
    "await proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n",
    "\n",
    "print(\"\\n✅ Workspace re-sync completed!\")\n",
    "print(\"\\n💡 The updated config is now available on all remote nodes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Updated File on Remote Nodes\n",
    "\n",
    "Let's confirm the updated config made it to the remote nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"📄 Reading updated config from rank 0:\")\n",
    "print(f\"{'-'*50}\")\n",
    "\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    remote_content = read_results[0]['content']\n",
    "    print(remote_content)\n",
    "    \n",
    "    # Verify it matches our local update\n",
    "    if \"learning_rate = 0.0005\" in remote_content and \"max_steps = 200\" in remote_content:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(\"\\n✅ SUCCESS! Remote config matches local changes:\")\n",
    "        print(\"  ✓ Learning rate: 0.001 → 0.0005\")\n",
    "        print(\"  ✓ Max steps: 100 → 200\")\n",
    "        print(\"  ✓ Sequence length: 1024 → 2048\")\n",
    "    else:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(\"\\n⚠️ Warning: Remote config may not have updated correctly\")\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")\n",
    "    print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Real-World Workflow Example\n",
    "\n",
    "Here's how you'd use workspace sync in a real training scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Iterative Training with Config Changes\n",
    "\n",
    "```python\n",
    "# 1. Initial training run\n",
    "await async_main(config)  # Train with initial settings\n",
    "\n",
    "# 2. Review results, decide to adjust learning rate\n",
    "# Edit local config file...\n",
    "\n",
    "# 3. Sync changes (< 1 second)\n",
    "await proc_mesh.sync_workspace(workspace=workspace)\n",
    "\n",
    "# 4. Re-run training with new config (no restart!)\n",
    "config = config_manager.parse_args(manual_args)  # Reload config\n",
    "await async_main(config)  # Train with updated settings\n",
    "\n",
    "# 5. Repeat as needed!\n",
    "```\n",
    "\n",
    "### Time Savings\n",
    "\n",
    "**Without Monarch:**\n",
    "- Change config: 1 min\n",
    "- Stop job: 1 min\n",
    "- Restart job: 5-10 min\n",
    "- **Total per iteration: ~7-12 min**\n",
    "\n",
    "**With Monarch:**\n",
    "- Change config: 1 min\n",
    "- Sync: < 1 sec\n",
    "- **Total per iteration: ~1 min**\n",
    "\n",
    "**10x faster iteration!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Syncing Multiple Files and Directories\n",
    "\n",
    "You can sync entire directory trees, not just single files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sync multiple directories\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a workspace with multiple directories\n",
    "multi_dir_workspace = Workspace(dirs=[\n",
    "    Path(\"/teamspace/studios/this_studio/configs\"),\n",
    "    Path(\"/teamspace/studios/this_studio/custom_modules\"),\n",
    "    Path(\"/teamspace/studios/this_studio/data_processors\"),\n",
    "])\n",
    "\n",
    "# Sync all directories at once\n",
    "# await proc_mesh.sync_workspace(workspace=multi_dir_workspace)\n",
    "\n",
    "print(\"\\n💡 Tip: You can sync entire project directories, not just config files!\")\n",
    "print(\"This enables hot-reloading of:\")\n",
    "print(\"  • Training scripts\")\n",
    "print(\"  • Model definitions\")\n",
    "print(\"  • Data preprocessing code\")\n",
    "print(\"  • Custom layers and modules\")\n",
    "print(\"  • And more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 Congratulations! 🎉\n",
    "\n",
    "You've mastered **workspace synchronization** with Monarch!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- Creating a Monarch `Workspace` for local directories\n",
    "- Syncing files to remote nodes with `proc_mesh.sync_workspace()`\n",
    "- Verifying synchronization across the cluster\n",
    "- Hot-reloading configs without job restarts\n",
    "- Real-world iterative training workflows\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **10x faster iteration** - No more waiting for job restarts\n",
    "- **Edit locally, run remotely** - Keep your familiar dev environment\n",
    "- **Sync is smart** - Only changed files are transferred\n",
    "- **Works with any files** - Configs, code, data processors, etc.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### 🐛 Studio 3: Interactive Debugging (Recommended Next)\n",
    "Learn advanced debugging techniques:\n",
    "- Set breakpoints in distributed actors\n",
    "- Debug specific ranks with `monarch debug`\n",
    "- Inspect and modify environment variables\n",
    "- Troubleshoot training issues interactively\n",
    "\n",
    "### 📚 Back to Studio 1\n",
    "Review the basics: [Studio 1: Getting Started](./studio_1_getting_started.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Try It Yourself!\n",
    "\n",
    "Before moving on, try modifying the config one more time:\n",
    "1. Change the batch size to 64\n",
    "2. Sync the workspace\n",
    "3. Verify the changes\n",
    "\n",
    "This workflow will become second nature!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}