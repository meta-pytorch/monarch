{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Notebook: TorchTitan Multi-Node Training with Monarch & Lightning SDK\n",
    "\n",
    "This notebook demonstrates how to run TorchTitan training using Monarch for distributed multi-node training on Lightning AI infrastructure.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/NB_Monarch_Lightning.svg\" alt=\"Monarch Lightning Architecture\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "<!-- Image size settings:\n",
    "  - Adjust 'width' attribute to control the diagram size (e.g., width=\"600\", width=\"1000\", or width=\"100%\")\n",
    "  - You can also use 'height' attribute instead (e.g., height=\"400\")\n",
    "  - Remove width/height attributes to display at original size\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This notebook provides a comprehensive guide to running distributed multi-node training using **Monarch** (Meta's distributed actor framework) with **TorchTitan** (PyTorch's large-scale LLM training library) on **Lightning AI** infrastructure. You'll learn how to set up, execute, debug, and manage distributed training workflows across multiple GPU nodes. \n",
    "\n",
    "While Part I & II are the core of this Notebook for setup and training; Part III is for users who are interested in Monarch's advanced features such as interactive distributed debugging, environment variable management, and code synchronization for workspaces between local node and remote nodes.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "**Part I: Environment Setup** *(Essential Prerequisites)*\n",
    "- Install TorchTitan - Set up PyTorch and TorchTitan for LLM training\n",
    "- Download Llama-3.1-8B Model Assets - Get model tokenizers from Hugging Face\n",
    "- Install Monarch - Install Meta's distributed actor framework\n",
    "- Setup Weights & Biases - Configure experiment tracking\n",
    "- Update Lightning SDK - Get the latest Lightning SDK features\n",
    "- Verify Installations - Confirm all dependencies are ready\n",
    "\n",
    "**Part II: Multi-Node Training** *(Core Training Workflow)*\n",
    "- Import Lightning SDK Components - Import required classes for multi-machine training\n",
    "- Configure Training Job Parameters - Set up nodes, GPUs, and network settings\n",
    "- Launch Multi-Node Training Job - Start distributed infrastructure on Lightning AI\n",
    "- Set Up Process Mesh - Initialize Monarch's distributed computing mesh\n",
    "- Define TorchTitan Trainer Actor - Create distributed training actor\n",
    "- Run TorchTitan Training - Execute Llama 3-8B training across nodes\n",
    "\n",
    "**Part III: Advanced Features** *(Distributed Development & Debugging)*\n",
    "\n",
    "1. **Environment Variable Management**\n",
    "   - Spawn Environment Variable Actor - Manage env vars across nodes\n",
    "   - Get/Set Environment Variables - Inspect and modify remote environments\n",
    "   - List Environment Variables - Query env vars by prefix\n",
    "\n",
    "2. **Workspace Synchronization** *(Hot-Reload Code & Configs)*\n",
    "   - Introduction to sync_workspace - Understanding workspace sync\n",
    "   - Content checker Actor for files - Define an Actor to check content\n",
    "   - Create Local Configuration - Set up training configs\n",
    "   - Sync to Remote Nodes - Propagate changes to workers\n",
    "   - Verify Synchronization - Confirm files are synced\n",
    "\n",
    "3. **Interactive Debugging with Breakpoints**\n",
    "   - Debugging Overview - Using pdb with distributed actors\n",
    "   - Define Debug Trainer - Create actor with breakpoints\n",
    "   - Spawn and Debug - Run interactive debugging session\n",
    "   - Debugger Commands - Learn monarch debug CLI commands\n",
    "\n",
    "**Part IV: Cleanup**\n",
    "- Stop Process Mesh - Gracefully shutdown distributed resources\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Monarch Actor**: Distributed computation unit that runs on remote nodes\n",
    "- **Process Mesh (ProcMesh)**: Network of processes across multiple nodes for distributed computing\n",
    "- **Endpoint**: Method decorator that makes actor methods callable remotely\n",
    "- **Workspace Sync**: Synchronize local code/config changes to remote worker nodes without restart\n",
    "- **Lightning MMT**: Multi-Machine Training orchestration on Lightning AI\n",
    "\n",
    "### Prerequisites\n",
    "- Lightning AI account with access to GPU machines (L40S recommended)\n",
    "- Hugging Face account with Llama model access\n",
    "- Basic understanding of distributed training concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Environment Setup\n",
    "\n",
    "Before running the notebook cells, ensure all dependencies are properly installed by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TorchTitan\n",
    "\n",
    "Clone the TorchTitan repository, install the nightly PyTorch build with CUDA 12.6 support, and install TorchTitan:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/pytorch/torchtitan.git\n",
    "cd torchtitan\n",
    "pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Llama-3-8B Model Assets\n",
    "\n",
    "Download the Llama-3.1-8B tokenizer from Hugging Face. You'll need a Hugging Face token with access to the Llama models:\n",
    "\n",
    "```bash\n",
    "python scripts/download_hf_assets.py \\\n",
    "    --repo_id meta-llama/Llama-3.1-8B \\\n",
    "    --assets tokenizer \\\n",
    "    --hf_token=YOUR_HUGGINGFACE_TOKEN_KEY\n",
    "```\n",
    "\n",
    "Replace `YOUR_HUGGINGFACE_TOKEN_KEY` with your actual Hugging Face token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Monarch\n",
    "\n",
    "Install Monarch from the GitHub repository following the Ubuntu installation instructions:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/meta-pytorch/monarch.git\n",
    "cd monarch\n",
    "# Follow the Ubuntu installation instructions from the repository\n",
    "```\n",
    "\n",
    "For detailed installation steps, visit: https://github.com/meta-pytorch/monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weights & Biases\n",
    "\n",
    "Check if wandb is installed. If not, install it and login:\n",
    "\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n",
    "\n",
    "Follow the prompts to authenticate with your wandb account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Lightning SDK\n",
    "\n",
    "The latest version of lightning SDK offers IP sharing between the client host and remote nodes. This features is being used in this Notebook.\n",
    "\n",
    "```bash\n",
    "pip install -U lightning_sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installations\n",
    "\n",
    "After completing the installation steps above, verify that TorchTitan and Monarch are properly installed:\n",
    "\n",
    "```python\n",
    "# Verify TorchTitan installation\n",
    "import torchtitan\n",
    "print(\"TorchTitan is installed successfully\")\n",
    "\n",
    "# Verify Monarch installation\n",
    "import monarch\n",
    "print(\"Monarch is installed successfully\")\n",
    "\n",
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "```\n",
    "\n",
    "If all imports succeed, you're ready to proceed with the training workflow below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Multi-Node Training with Monarch and Lightning\n",
    "\n",
    "Now that the environment is set up, we can proceed with configuring and launching the distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lightning SDK Components\n",
    "\n",
    "Import the necessary classes from Lightning SDK to manage multi-machine training jobs, including `Machine` for hardware specifications, `MMT` for multi-machine training orchestration, and `Studio` for workspace management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Job Parameters\n",
    "\n",
    "Set up the configuration for the multi-node training job, including the number of nodes (2), GPUs per node (8), teamspace name, username, and port range for worker node communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "NUM_NODES = 16\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
    "USER = \"meta-ai\"  # Replace with your username\n",
    "MMT_JOB_NAME = f\"Monarch-v0-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "# Remote allowed port range for worker nodes\n",
    "REMOTE_ALLOWED_PORT_RANGE = \"26601..26611\"\n",
    "\n",
    "# To force Monarch to use V0 for this Notebook (This will be removed in the future)\n",
    "os.environ[\"MONARCH_V0_WORKAROUND_DO_NOT_USE\"] = \"1\"\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MMT Job Launch Function\n",
    "\n",
    "Create a function to launch a multi-machine training (MMT) job using Lightning SDK. This function installs the MMT plugin, configures the machine type (L40S GPUs), sets environment variables for CUDA devices and Monarch configurations, and returns the job handle and studio instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    # Install the MMT plugin befor running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{NUM_GPUS}\")\n",
    "\n",
    "     # Machine with L40 GPUs\n",
    "    # machine_type = getattr(Machine, f\"L4_X_{NUM_GPUS}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
    "\n",
    "    job = MMT.run(\n",
    "        command=\"process_allocator\",\n",
    "        name=MMT_JOB_NAME,\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible # TODO: Should make this one dynamic\n",
    "            \"MONARCH_FILE_LOG\": \"debug\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\": REMOTE_ALLOWED_PORT_RANGE,\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\": \"true\",\n",
    "            \"WORKSPACE_DIR\": \"/tmp\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    # Monitor job status\n",
    "    return job, studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Multi-Node Training Job\n",
    "\n",
    "Execute the `launch_mmt_job` function with the specified number of nodes, teamspace, and user credentials. This starts the distributed training infrastructure and provides commands for monitoring and stopping the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching MMT job with 16 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/Multi-Node-Monarch-Titan-Scale-16_nodes-port_override?app_id=mmt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: Multi-Node-Monarch-Titan-Scale-16_nodes-port_override\n",
      "Job status: Pending\n",
      "Job launched. You can monitor it using: job.status\n",
      "To stop the job: job.stop()\n",
      "To clean up: studio.stop()\n"
     ]
    }
   ],
   "source": [
    "# Launch the job\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor jobs in the MMT Plugin\n",
    "\n",
    "When user initiate a job, they can monitor the status of the job through the MMT plugin.\n",
    "Running the cell above initaties the requested number of nodes on the lightning cluster.\n",
    "The user may see different setups for the nodes like this:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/setup_status.png\" alt=\"setup status\" width=\"800\"/>\n",
    "</div>\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/nodes_pending.png\" alt=\"nodes pending\" width=\"1200\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Once nodes are available through the lightning, the SDK will take care of snapshot-ing your environment, setup the nodes, and copy the corresponded data:\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/nodes_ready.png\" alt=\"nodes ready\" width=\"1200\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Process Mesh from Job\n",
    "\n",
    "Initialize the Monarch process mesh using the launched Lightning job. This creates the distributed computing mesh that connects all nodes and GPUs for coordinated training.\n",
    "\n",
    "Before running the cell below, please make sure that the `process_allocator` process from Monarch is running on your requested nodes! You can confirm that by taking a look at the MMT SDK:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/process_allocator_log.png\" alt=\"process_allocator_log\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File /tmp/worker_nodes.txt not found\n",
      "Extracted IP addresses:\n",
      "\n",
      "IP set: set()\n",
      "ip_addresses_list=['3.150.74.121', '3.148.30.121', '3.149.152.201', '3.139.30.73', '3.130.134.76', '3.150.253.90', '3.18.136.106', '3.149.202.134', '3.133.10.50', '3.142.183.138', '18.217.116.20', '3.134.162.180', '3.20.57.208', '3.151.0.192', '18.216.236.74', '18.223.178.27']\n",
      "ip_addresses_set={'3.151.0.192', '3.18.136.106', '3.133.10.50', '18.223.178.27', '3.150.74.121', '3.149.152.201', '3.20.57.208', '3.148.30.121', '3.150.253.90', '18.217.116.20', '18.216.236.74', '3.134.162.180', '3.130.134.76', '3.139.30.73', '3.142.183.138', '3.149.202.134'}\n",
      "IP addresses are available: True\n",
      "private_master_host_ip_address='10.192.12.151'\n",
      "public_master_host_ip_address='54.209.46.214'\n",
      "tcp!3.151.0.192:26600 tcp!3.18.136.106:26600 tcp!3.133.10.50:26600 tcp!18.223.178.27:26600 tcp!3.150.74.121:26600 tcp!3.149.152.201:26600 tcp!3.20.57.208:26600 tcp!3.148.30.121:26600 tcp!3.150.253.90:26600 tcp!18.217.116.20:26600 tcp!18.216.236.74:26600 tcp!3.134.162.180:26600 tcp!3.130.134.76:26600 tcp!3.139.30.73:26600 tcp!3.142.183.138:26600 tcp!3.149.202.134:26600\n",
      "AllocHandle(_hy_alloc=<monarch._rust_bindings.monarch_hyperactor.pytokio.Shared object at 0x70f4f81ccc70>, _extent={'hosts': 16, 'gpus': 8}, _stream_logs=True, _allocator=<monarch._src.actor.allocator.RemoteAllocator object at 0x70f4f8f16db0>, _constraints=<monarch._rust_bindings.monarch_hyperactor.alloc.AllocConstraints object at 0x70f4f87017a0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: UserWarning: The AllocSpec passed to RemoteAllocator.allocate has transport unix, but the transport from the remote process alloc initializer is tcp(Hostname). This will soon be an error unless you explicitly configure monarch's default transport to tcp(Hostname). The current default transport is unix.\n"
     ]
    }
   ],
   "source": [
    "from utils.mesh_utils import setup_proc_mesh_from_job\n",
    "\n",
    "proc_mesh = setup_proc_mesh_from_job(job, NUM_NODES, NUM_GPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Hero - Run TorchTitan using Monarch for Llama 3 - 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Job Name Helper\n",
    "\n",
    "Define a utility function to generate a unique job name based on the username, number of hosts, and GPUs per host. This helps identify and track different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch-alisol-hosts16-gpus8\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TorchTitan Trainer Actor\n",
    "\n",
    "Create the `TitanTrainerWrapper` class, a Monarch Actor that wraps TorchTitan's training functionality. This actor handles initialization, training execution, checkpointing, and cleanup of the distributed training process across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert (\n",
    "                    # config.checkpoint.enable_checkpoint\n",
    "                    config.checkpoint.enable\n",
    "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0, )\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Async Main Training Function\n",
    "\n",
    "Set up the main asynchronous function that orchestrates the distributed training. This function configures the environment for distributed execution, spawns trainer actors across the process mesh, and initiates the training workflow. The reason that this function is defined as async is becuase of those call of endpoints where need to be awaited. This makes sure that coordination of operations across multiple machines are done asynchronously rather than blocking the main thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.tools.network import AddrType\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    \"\"\"\n",
    "    # if use_ipaddr is not passed, then default is IPv6 for MASTER_ADDR\n",
    "    \"\"\"\n",
    "    await setup_env_for_distributed(proc_mesh, use_ipaddr=AddrType.IPv4)\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Logger and Run Training\n",
    "\n",
    "Configure the TorchTitan logger and parse training arguments including model configuration file, tokenizer path, dataset location, number of training steps, and output directory. Then execute the asynchronous training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[titan] 2025-10-20 05:16:23,787 - root - WARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.\n",
      "JobConfig(job=Job(config_file='/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml', dump_folder='/teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8', description='Llama 3 8B training', print_config=False, custom_config_module=''), profiling=Profiling(enable_profiling=True, save_traces_folder='profile_trace', profile_freq=100, profiler_active=1, profiler_warmup=3, enable_memory_snapshot=False, save_memory_snapshot_folder='memory_snapshot'), metrics=Metrics(log_freq=1, enable_tensorboard=True, disable_color_printing=False, save_tb_folder='tb', save_for_all_ranks=False, enable_wandb=True), model=Model(name='llama3', flavor='8B', hf_assets_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', tokenizer_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', converters=[], print_after_conversion=False), optimizer=Optimizer(name='AdamW', lr=0.0003, beta1=0.9, beta2=0.95, eps=1e-08, weight_decay=0.1, implementation='fused', early_step_in_backward=False), lr_scheduler=LRScheduler(warmup_steps=200, decay_ratio=None, decay_type='linear', min_lr_factor=0.0), training=Training(dataset='c4_test', dataset_path='/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test', local_batch_size=1, global_batch_size=-1, seq_len=1024, max_norm=1.0, steps=25, enable_cpu_offload=False, dtype='float32', mixed_precision_param='bfloat16', mixed_precision_reduce='float32', gc_freq=50, gc_debug=False, seed=None, deterministic=False, debug_moe_force_load_balance=False), parallelism=Parallelism(data_parallel_replicate_degree=1, enable_compiled_autograd=False, data_parallel_shard_degree=-1, fsdp_reshard_after_forward='default', tensor_parallel_degree=1, disable_loss_parallel=False, enable_async_tensor_parallel=False, pipeline_parallel_degree=1, module_fqns_per_model_part=None, pipeline_parallel_first_stage_less_layers=1, pipeline_parallel_last_stage_less_layers=1, pipeline_parallel_layers_per_stage=None, pipeline_parallel_schedule='1F1B', pipeline_parallel_schedule_csv='', pipeline_parallel_microbatch_size=1, context_parallel_degree=1, context_parallel_rotate_method='allgather', expert_parallel_degree=1, expert_tensor_parallel_degree=1), checkpoint=Checkpoint(enable=False, enable_ft_dataloader_checkpoints=True, folder='checkpoint', interval=500, initial_load_path=None, initial_load_model_only=True, initial_load_in_hf=False, initial_load_in_hf_quantized=False, last_save_model_only=True, last_save_in_hf=False, export_dtype='float32', async_mode='disabled', keep_latest_k=10, load_step=-1, exclude_from_loading=[], enable_first_step_checkpoint=False, create_seed_checkpoint=False, load_only=False), activation_checkpoint=ActivationCheckpoint(mode='selective', selective_ac_option='op', per_op_sac_force_recompute_mm_shapes_by_fqns=['moe.router.gate'], early_stop=False, memory_budget=0.5, visualize_memory_budget_pareto=False), compile=Compile(enable=False, components=['model', 'loss'], backend='inductor'), quantize=Quantize(linear=QuantizedLinear(float8=Float8Linear(enable_fsdp_float8_all_gather=False, precompute_float8_dynamic_scale_for_fsdp=False, recipe_name=None, filter_fqns=['output'], emulate=False), mx=MXLinear(mxfp8_dim1_cast_kernel_choice='triton', recipe_name='mxfp8_cublas', filter_fqns=['output'])), grouped_mm=QuantizedGroupedMM(float8=Float8GroupedMM(fqns=[]), mx=MXGroupedMM(recipe_name='mxfp8', fqns=[]))), comm=Comm(init_timeout_seconds=300, train_timeout_seconds=100, trace_buf_size=20000, save_traces_folder='comm_traces', save_traces_file_prefix='rank_'), memory_estimation=MemoryEstimation(enable=False, disable_fake_mode=False), fault_tolerance=FaultTolerance(enable=False, process_group='gloo', process_group_timeout_ms=10000, replica_id=0, group_size=0, min_replica_size=1, semi_sync_method=None), experimental=Experimental(custom_import='', custom_args_module=''), validation=Validation(enable=False, dataset='c4_validation', dataset_path=None, local_batch_size=8, seq_len=2048, freq=500, steps=1200))\n",
      "Spawning meshes on monarch-alisol-hosts16-gpus8\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:14:41) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Initializing actor: 16 current_rank()={'hosts': 2/16, 'gpus': 0/8} socket.gethostname()='ip-10-192-11-77'\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:27) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:14:41) >>>\u001b[0m\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Starting training\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Starting job: Llama 3 8B training\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Building 1-D device mesh with ['dp_shard'], [128]\n",
      "\u001b[33m[128 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/device_mesh.py:788: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[128 similar log lines]\u001b[0m   warnings.warn(\n",
      "\u001b[33m[128 similar log lines]\u001b[0m [GC] Initial GC collection took 0.00 seconds\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:30) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:27) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m Initializing actor: 22 current_rank()={'hosts': 2/16, 'gpus': 6/8} socket.gethostname()='ip-10-192-11-77'\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:30) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:30) >>>\u001b[0m\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Loading tokenizer from tokenizer.json\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Preparing c4_test dataset from /teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 2000 examples [00:00, 89312.72 examples/s]\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, rope_scaling_args=RoPEScalingArgs(scaling_factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192), max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)\n",
      "\u001b[33m[127 similar log lines]\u001b[0m CUDA capacity: NVIDIA L40S with 44.64GiB memory\n",
      "\u001b[33m[159 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[34mModel llama3 8B \u001b[31msize: 8,030,261,248 total parameters\u001b[39m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m Applied selective activation checkpointing to the model\n",
      "\u001b[33m[127 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/device_mesh.py:788: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[129 similar log lines]\u001b[0m   warnings.warn(\n",
      "\u001b[33m[127 similar log lines]\u001b[0m Applied FSDP to the model\n",
      "\u001b[33m[2 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "\u001b[33m[104 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/init.py:119: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/zeus/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1487.)\n",
      "\u001b[33m[104 similar log lines]\u001b[0m   tensor.erfinv_()\n",
      "\u001b[33m[29 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
      "\u001b[33m[28 similar log lines]\u001b[0m CUDA memory usage for model: 0.25GiB(0.56%)\n",
      "\u001b[33m[26 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
      "\u001b[33m[26 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
      "\u001b[33m[24 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
      "\u001b[33m[22 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 128, gradient accumulation steps 1, sequence length 1024, total steps 25 (warmup 200)\n",
      "\u001b[33m[19 similar log lines]\u001b[0m Training starts at step 1\n",
      "\u001b[33m[15 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/profile_trace\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Currently logged in as: a-shamsoshoara (a-shamsoshoara-m) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:33) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:33) >>>\u001b[0m\n",
      "\u001b[33m[105 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 128, gradient accumulation steps 1, sequence length 1024, total steps 25 (warmup 200)\n",
      "\u001b[33m[112 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/profile_trace\n",
      "\u001b[33m[98 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
      "\u001b[33m[95 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
      "\u001b[33m[108 similar log lines]\u001b[0m Training starts at step 1\n",
      "\u001b[33m[101 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
      "\u001b[33m[99 similar log lines]\u001b[0m CUDA memory usage for model: 0.25GiB(0.56%)\n",
      "\u001b[33m[103 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
      "\u001b[33m[101 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:33) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:33) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Tracking run with wandb version 0.22.2\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run data is saved locally in /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/tb/20251020-0516/wandb/run-20251020_051633-kog9t67d\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Syncing run easy-waterfall-51\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run at https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/kog9t67d\n",
      "\u001b[33m[1 similar log lines]\u001b[0m WandB logging enabled\n",
      "\u001b[33m[1 similar log lines]\u001b[0m TensorBoard logging enabled. Logs will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/tb/20251020-0516\n",
      "\u001b[33m[1 similar log lines]\u001b[0m CUDA capacity: NVIDIA L40S with 44.64GiB memory\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[34mModel llama3 8B \u001b[31msize: 8,030,261,248 total parameters\u001b[39m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Applied selective activation checkpointing to the model\n",
      "\u001b[33m[1 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/device_mesh.py:788: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[1 similar log lines]\u001b[0m   warnings.warn(\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Applied FSDP to the model\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
      "\u001b[33m[1 similar log lines]\u001b[0m CUDA memory usage for model: 0.25GiB(0.56%)\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
      "\u001b[33m[1 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 128, gradient accumulation steps 1, sequence length 1024, total steps 25 (warmup 200)\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Training starts at step 1\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/profile_trace\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:36) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:36) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/device_mesh.py:788: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:53) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:53) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/device_mesh.py:788: UserWarning: Slicing a flattened dim from root mesh will be deprecated in PT 2.11. Users need to bookkeep the flattened mesh directly. \n",
      "\u001b[33m[128 similar log lines]\u001b[0m   warnings.warn(\n",
      "\u001b[33m[128 similar log lines]\u001b[0m \u001b[31mstep:  1  \u001b[32mloss: 12.2511  \u001b[38;2;180;60;0mgrad_norm:  3.7981  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 47  \u001b[36mtflops: 2.21  \u001b[35mmfu: 0.71%\u001b[39m\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:16:56) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:56) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.3997  \u001b[38;2;180;60;0mgrad_norm:  4.5709  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:10) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:10) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.3997  \u001b[38;2;180;60;0mgrad_norm:  4.5709  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:13) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:13) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 12.1637  \u001b[38;2;180;60;0mgrad_norm: 58.1643  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:27) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:27) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 12.1637  \u001b[38;2;180;60;0mgrad_norm: 58.1643  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[9 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:30) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:30) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 13.2350  \u001b[38;2;180;60;0mgrad_norm: 50.7540  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:44) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:44) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 13.2350  \u001b[38;2;180;60;0mgrad_norm: 50.7540  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[21 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:17:47) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:17:47) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.5873  \u001b[38;2;180;60;0mgrad_norm: 12.7209  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:01) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:01) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.5873  \u001b[38;2;180;60;0mgrad_norm: 12.7209  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[21 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:04) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:04) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 12.4519  \u001b[38;2;180;60;0mgrad_norm: 11.3216  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:18) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:18) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 12.4519  \u001b[38;2;180;60;0mgrad_norm: 11.3216  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[21 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:21) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:21) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 14.5130  \u001b[38;2;180;60;0mgrad_norm: 67.7407  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:35) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:35) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 14.5130  \u001b[38;2;180;60;0mgrad_norm: 67.7407  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[22 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:38) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:38) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 12.5781  \u001b[38;2;180;60;0mgrad_norm: 30.1054  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:52) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:52) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 12.5781  \u001b[38;2;180;60;0mgrad_norm: 30.1054  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[24 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:18:55) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:18:55) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 11.3309  \u001b[38;2;180;60;0mgrad_norm: 14.7525  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:09) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:09) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 11.3309  \u001b[38;2;180;60;0mgrad_norm: 14.7525  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[24 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:12) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:12) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss: 10.5148  \u001b[38;2;180;60;0mgrad_norm:  5.6056  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:26) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:26) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss: 10.5148  \u001b[38;2;180;60;0mgrad_norm:  5.6056  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[22 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:29) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:29) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss: 10.0892  \u001b[38;2;180;60;0mgrad_norm: 11.5532  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:43) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:43) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss: 10.0892  \u001b[38;2;180;60;0mgrad_norm: 11.5532  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[18 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:19:46) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:19:46) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.5798  \u001b[38;2;180;60;0mgrad_norm:  6.9032  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:00) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:00) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.5798  \u001b[38;2;180;60;0mgrad_norm:  6.9032  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[19 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:03) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:03) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss:  8.9717  \u001b[38;2;180;60;0mgrad_norm:  4.7648  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:17) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:17) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss:  8.9717  \u001b[38;2;180;60;0mgrad_norm:  4.7648  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[20 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:20) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:20) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss:  8.3563  \u001b[38;2;180;60;0mgrad_norm: 11.2116  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:33) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:33) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss:  8.3563  \u001b[38;2;180;60;0mgrad_norm: 11.2116  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[23 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:36) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:36) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.6429  \u001b[38;2;180;60;0mgrad_norm: 18.0148  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:50) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:50) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.6429  \u001b[38;2;180;60;0mgrad_norm: 18.0148  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:20:53) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:20:53) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.4340  \u001b[38;2;180;60;0mgrad_norm: 14.1969  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:07) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:07) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.4340  \u001b[38;2;180;60;0mgrad_norm: 14.1969  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[23 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:10) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:10) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.0953  \u001b[38;2;180;60;0mgrad_norm:  4.8753  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:24) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:24) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.0953  \u001b[38;2;180;60;0mgrad_norm:  4.8753  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[20 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:27) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:27) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  7.9582  \u001b[38;2;180;60;0mgrad_norm:  5.3509  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:41) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:41) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  7.9582  \u001b[38;2;180;60;0mgrad_norm:  5.3509  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[19 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:44) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:44) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  7.6446  \u001b[38;2;180;60;0mgrad_norm: 15.6095  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:21:58) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:21:58) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  7.6446  \u001b[38;2;180;60;0mgrad_norm: 15.6095  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[19 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:01) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:01) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss:  7.8602  \u001b[38;2;180;60;0mgrad_norm: 11.7412  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:15) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:15) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss:  7.8602  \u001b[38;2;180;60;0mgrad_norm: 11.7412  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[20 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:18) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:18) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  7.6879  \u001b[38;2;180;60;0mgrad_norm:  5.6927  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:32) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:32) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  7.6879  \u001b[38;2;180;60;0mgrad_norm:  5.6927  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[24 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:35) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:35) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  7.5203  \u001b[38;2;180;60;0mgrad_norm:  4.5345  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:49) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:49) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  7.5203  \u001b[38;2;180;60;0mgrad_norm:  4.5345  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[30 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:22:52) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:22:52) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  7.5767  \u001b[38;2;180;60;0mgrad_norm:  6.0536  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:06) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:06) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  7.5767  \u001b[38;2;180;60;0mgrad_norm:  6.0536  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 61  \u001b[36mtflops: 2.83  \u001b[35mmfu: 0.91%\u001b[39m\n",
      "\u001b[33m[14 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:09) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:09) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  7.4580  \u001b[38;2;180;60;0mgrad_norm:  3.3011  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:23) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:23) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  7.4580  \u001b[38;2;180;60;0mgrad_norm:  3.3011  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.82  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:26) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:26) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  7.4618  \u001b[38;2;180;60;0mgrad_norm:  4.8066  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:40) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:16:30) >>>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[126 similar log lines]\u001b[0m Done training\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:43) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 05:23:40) >>>\u001b[0m\n",
      "\u001b[33m[127 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  7.4618  \u001b[38;2;180;60;0mgrad_norm:  4.8066  \u001b[38;2;54;234;195mmemory: 12.80GiB(28.69%)  \u001b[34mtps: 60  \u001b[36mtflops: 2.81  \u001b[35mmfu: 0.90%\u001b[39m\n",
      "\u001b[33m[255 similar log lines]\u001b[0m Training completed\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Training starts at step 26\n",
      "\u001b[33m[128 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts16-gpus8/profile_trace\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Sleeping 2 seconds for other ranks to complete\n",
      "\u001b[33m[126 similar log lines]\u001b[0m Process group destroyed.\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:43) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:43) >>>\u001b[0m\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Done training\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:46) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-20 05:23:43) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Training completed\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: updating run metadata\n",
      "\u001b[33m[3 similar log lines]\u001b[0m wandb: \n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run history:\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                    grad_norm ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[2 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[33m[2 similar log lines]\u001b[0m wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[33m[3 similar log lines]\u001b[0m wandb:       memory/max_reserved(%) ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[2 similar log lines]\u001b[0m wandb:                           +7 ...\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run summary:\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                    grad_norm 4.80659\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss 7.4618\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_max_loss 10.02059\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr 0.0003\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Process group destroyed.\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:         memory/max_active(%) 12.94759\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_active(GiB) 5.77951\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_reserved(%) 28.68578\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/max_reserved(GiB) 12.80469\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/num_alloc_retries 0\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:              memory/num_ooms 0\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run easy-waterfall-51 at: https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/kog9t67d\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Find logs at: ./torchtitan/outputs/monarch-alisol-hosts16-gpus8/tb/20251020-0516/wandb/run-20251020_051633-kog9t67d/logs\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-20 05:23:46) <<<\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "        \"--model.tokenizer-path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "        \"--training.steps\",\n",
    "        \"25\",\n",
    "        \"--training.dataset_path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "        \"--job.dump_folder\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "        \"--training.seq_len\",\n",
    "        \"1024\",\n",
    "        # \"8192\",\n",
    "    ]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâüéâ Congratulations!!!! üéâüéâ You just ran the interactive distributed training for Llama-3 model in a Notebook using Monarch actors and Lightning setup!**\n",
    "\n",
    "This already gives the user lots of flexibilities such as changing the configurations and launching another training without iniatiating another job or set of nodes; or experiencing the logging aggregation using Monarch.\n",
    "\n",
    "However, a curious user can dig more into advanced features of Monarch in Part III. Monarch offers features such as interactive distributed debugging while your training is running on mutliple nodes and ranks. Another feature is the `workspace_sync` where users can update packages, environments and files and sync them with remote nodes. Without Monarch, users may need to re-initiate their launches which usually takes lots of times. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Part III: Advanced Features (Distributed Development & Debugging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variable Management with Remote Actors\n",
    "\n",
    "Spawn an actor that can interact with environment variables on remote nodes. This is useful for debugging, configuration management, and runtime environment inspection across the distributed system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.actor import Actor, endpoint, current_rank\n",
    "import os\n",
    "import socket\n",
    "\n",
    "class EnvVarActor(Actor):\n",
    "    \"\"\"Actor for managing environment variables on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def get_env(self, var_name: str) -> dict:\n",
    "        \"\"\"Get an environment variable value from the remote node.\"\"\"\n",
    "        value = os.environ.get(var_name)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": value\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def set_env(self, var_name: str, var_value: str) -> dict:\n",
    "        \"\"\"Set an environment variable on the remote node.\"\"\"\n",
    "        os.environ[var_name] = var_value\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": var_value,\n",
    "            \"status\": \"set\"\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def list_env_vars(self, prefix: str = \"\") -> dict:\n",
    "        \"\"\"List all environment variables matching a prefix.\"\"\"\n",
    "        matching_vars = {k: v for k, v in os.environ.items() if k.startswith(prefix)}\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"matching_vars\": matching_vars,\n",
    "            \"count\": len(matching_vars)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn the Environment Variable Actor\n",
    "\n",
    "Spawn the `EnvVarActor` across all nodes in the process mesh. Each node will have an instance that can be used to inspect and modify its local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvVarActor spawned across all nodes\n"
     ]
    }
   ],
   "source": [
    "# Spawn the environment variable actor across all nodes\n",
    "env_actor = proc_mesh.spawn(\"env_actor\", EnvVarActor)\n",
    "print(\"EnvVarActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Variables from Remote Nodes\n",
    "\n",
    "Query environment variables from all remote nodes. This example retrieves the `CUDA_VISIBLE_DEVICES` variable that was set during job initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA_VISIBLE_DEVICES on all nodes:\n",
      "  Host 0 gpus 0  Rank 0 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 1  Rank 1 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 2  Rank 2 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 3  Rank 3 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 4  Rank 4 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 5  Rank 5 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 6  Rank 6 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 0 gpus 7  Rank 7 (ip-10-192-11-251): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 0  Rank 8 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 1  Rank 9 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 2  Rank 10 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 3  Rank 11 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 4  Rank 12 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 5  Rank 13 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 6  Rank 14 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 1 gpus 7  Rank 15 (ip-10-192-11-128): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 0  Rank 16 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 1  Rank 17 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 2  Rank 18 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 3  Rank 19 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 4  Rank 20 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 5  Rank 21 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 6  Rank 22 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 2 gpus 7  Rank 23 (ip-10-192-11-77): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 0  Rank 24 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 1  Rank 25 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 2  Rank 26 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 3  Rank 27 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 4  Rank 28 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 5  Rank 29 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 6  Rank 30 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 3 gpus 7  Rank 31 (ip-10-192-11-28): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 0  Rank 32 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 1  Rank 33 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 2  Rank 34 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 3  Rank 35 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 4  Rank 36 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 5  Rank 37 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 6  Rank 38 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 4 gpus 7  Rank 39 (ip-10-192-11-124): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 0  Rank 40 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 1  Rank 41 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 2  Rank 42 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 3  Rank 43 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 4  Rank 44 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 5  Rank 45 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 6  Rank 46 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 5 gpus 7  Rank 47 (ip-10-192-11-35): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 0  Rank 48 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 1  Rank 49 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 2  Rank 50 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 3  Rank 51 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 4  Rank 52 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 5  Rank 53 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 6  Rank 54 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 6 gpus 7  Rank 55 (ip-10-192-11-103): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 0  Rank 56 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 1  Rank 57 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 2  Rank 58 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 3  Rank 59 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 4  Rank 60 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 5  Rank 61 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 6  Rank 62 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 7 gpus 7  Rank 63 (ip-10-192-11-176): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 0  Rank 64 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 1  Rank 65 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 2  Rank 66 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 3  Rank 67 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 4  Rank 68 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 5  Rank 69 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 6  Rank 70 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 8 gpus 7  Rank 71 (ip-10-192-11-186): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 0  Rank 72 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 1  Rank 73 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 2  Rank 74 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 3  Rank 75 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 4  Rank 76 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 5  Rank 77 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 6  Rank 78 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 9 gpus 7  Rank 79 (ip-10-192-11-147): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 0  Rank 80 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 1  Rank 81 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 2  Rank 82 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 3  Rank 83 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 4  Rank 84 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 5  Rank 85 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 6  Rank 86 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 10 gpus 7  Rank 87 (ip-10-192-11-151): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 0  Rank 88 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 1  Rank 89 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 2  Rank 90 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 3  Rank 91 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 4  Rank 92 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 5  Rank 93 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 6  Rank 94 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 11 gpus 7  Rank 95 (ip-10-192-11-58): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 0  Rank 96 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 1  Rank 97 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 2  Rank 98 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 3  Rank 99 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 4  Rank 100 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 5  Rank 101 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 6  Rank 102 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 12 gpus 7  Rank 103 (ip-10-192-11-9): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 0  Rank 104 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 1  Rank 105 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 2  Rank 106 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 3  Rank 107 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 4  Rank 108 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 5  Rank 109 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 6  Rank 110 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 13 gpus 7  Rank 111 (ip-10-192-11-211): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 0  Rank 112 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 1  Rank 113 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 2  Rank 114 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 3  Rank 115 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 4  Rank 116 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 5  Rank 117 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 6  Rank 118 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 14 gpus 7  Rank 119 (ip-10-192-11-89): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 0  Rank 120 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 1  Rank 121 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 2  Rank 122 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 3  Rank 123 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 4  Rank 124 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 5  Rank 125 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 6  Rank 126 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n",
      "  Host 15 gpus 7  Rank 127 (ip-10-192-11-207): 0,1,2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "# Get an environment variable from all nodes\n",
    "results = await env_actor.get_env.call(\"CUDA_VISIBLE_DEVICES\")\n",
    "print(\"\\nCUDA_VISIBLE_DEVICES on all nodes:\")\n",
    "for result in results:\n",
    "    if len(result) > 1:\n",
    "        print(f\"  Host {result[0].get('hosts', '?')} gpus {result[0].get('gpus', '?')}  Rank {result[1].get('rank', '?')} ({result[1].get('hostname', '?')}): {result[1].get('value', '?')}\")\n",
    "    else:\n",
    "        print(f\"  Rank {result.get('rank', '?')} ({result.get('hostname', '?')}): {result.get('value', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables on Remote Nodes\n",
    "\n",
    "Set a custom environment variable on all remote nodes and verify it was set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom environment variable on all nodes\n",
    "set_results = await env_actor.set_env.call(\"CUSTOM_VAR\", \"test_value_123\")\n",
    "print(\"\\nSetting CUSTOM_VAR on all nodes:\")\n",
    "for result in set_results:\n",
    "    if len(result) > 1:\n",
    "        print(f\"  Rank {result[1]['rank']} ({result[1]['hostname']}): {result[1]['status']} - {result[1]['value']}\")\n",
    "    else:\n",
    "        print(f\"  Rank {result['rank']} ({result['hostname']}): {result['status']} - {result['value']}\")\n",
    "\n",
    "# Verify the variable was set by reading it back\n",
    "verify_results = await env_actor.get_env.call(\"CUSTOM_VAR\")\n",
    "print(\"\\nVerifying CUSTOM_VAR on all nodes:\")\n",
    "for result in verify_results:\n",
    "    if len(result) > 1:\n",
    "        print(f\"  Rank {result[1]['rank']} ({result[1]['hostname']}): {result[1]['value']}\")\n",
    "    else:\n",
    "        print(f\"  Rank {result['rank']} ({result['hostname']}): {result['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Environment Variables with Prefix\n",
    "\n",
    "List all environment variables that match a specific prefix (e.g., all CUDA-related or MONARCH-related variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA-related environment variables on all nodes:\n",
      "\n",
      "  Rank 0 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 1 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 2 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 3 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 4 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 5 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 6 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 7 (ip-10-192-11-251) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 8 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 9 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 10 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 11 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 12 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 13 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 14 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 15 (ip-10-192-11-128) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 16 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 17 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 18 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 19 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 20 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 21 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 22 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 23 (ip-10-192-11-77) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 24 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 25 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 26 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 27 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 28 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 29 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 30 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 31 (ip-10-192-11-28) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 32 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 33 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 34 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 35 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 36 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 37 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 38 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 39 (ip-10-192-11-124) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 40 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 41 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 42 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 43 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 44 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 45 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 46 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 47 (ip-10-192-11-35) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 48 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 49 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 50 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 51 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 52 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 53 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 54 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 55 (ip-10-192-11-103) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 56 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 57 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 58 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 59 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 60 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 61 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 62 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 63 (ip-10-192-11-176) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 64 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 65 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 66 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 67 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 68 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 69 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 70 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 71 (ip-10-192-11-186) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 72 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 73 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 74 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 75 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 76 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 77 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 78 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 79 (ip-10-192-11-147) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 80 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 81 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 82 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 83 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 84 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 85 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 86 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 87 (ip-10-192-11-151) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 88 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 89 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 90 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 91 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 92 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 93 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 94 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 95 (ip-10-192-11-58) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 96 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 97 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 98 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 99 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 100 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 101 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 102 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 103 (ip-10-192-11-9) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 104 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 105 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 106 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 107 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 108 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 109 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 110 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 111 (ip-10-192-11-211) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 112 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 113 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 114 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 115 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 116 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 117 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 118 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 119 (ip-10-192-11-89) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 120 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 121 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 122 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 123 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 124 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 125 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 126 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "\n",
      "  Rank 127 (ip-10-192-11-207) - 3 variables:\n",
      "    CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\n",
      "    CUDA_VERSION=12.6.3\n",
      "    CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "# List all environment variables starting with \"CUDA\"\n",
    "list_results = await env_actor.list_env_vars.call(\"CUDA\")\n",
    "print(\"\\nCUDA-related environment variables on all nodes:\")\n",
    "for result in list_results:\n",
    "    if len(result) > 1:\n",
    "        print(f\"\\n  Rank {result[1]['rank']} ({result[1]['hostname']}) - {result[1]['count']} variables:\")\n",
    "        for var_name, var_value in result[1]['matching_vars'].items():\n",
    "            print(f\"    {var_name}={var_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workspace Synchronization with `sync_workspace`\n",
    "\n",
    "When working with distributed training, you often need to modify configuration files, training scripts, or other code locally and sync those changes to remote worker nodes without restarting the entire job. Monarch's `proc_mesh.sync_workspace()` enables this workflow.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Make changes locally** - Edit files in your local workspace (e.g., configuration files, training scripts)\n",
    "2. **Call `sync_workspace()`** - Synchronize changes to all remote worker nodes\n",
    "3. **Continue execution** - The updated files are immediately available on all nodes\n",
    "\n",
    "This is particularly useful for:\n",
    "- Tweaking hyperparameters in configuration files\n",
    "- Updating training schedules\n",
    "- Modifying data processing logic\n",
    "- Hot-reloading code changes without job restart\n",
    "\n",
    "Let's see a practical example using TorchTitan training configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor to Check File Contents\n",
    "\n",
    "First, create an actor that can read and verify file contents on remote nodes. This will help us confirm that files are properly synchronized across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileCheckerActor(Actor):\n",
    "    \"\"\"Actor to read and verify file contents on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def read_file(self, file_path: str) -> dict:\n",
    "        \"\"\"Read a file and return its contents.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"exists\": True,\n",
    "                \"size\": len(content)\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": \"File not found\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    @endpoint\n",
    "    def file_exists(self, file_path: str) -> dict:\n",
    "        \"\"\"Check if a file exists on the remote node.\"\"\"\n",
    "        exists = os.path.exists(file_path)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"file_path\": file_path,\n",
    "            \"exists\": exists\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn File Checker Actor\n",
    "\n",
    "Spawn the file checker actor across all nodes to verify file synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileCheckerActor spawned across all nodes\n"
     ]
    }
   ],
   "source": [
    "# Spawn the file checker actor\n",
    "file_checker = proc_mesh.spawn(\"file_checker\", FileCheckerActor)\n",
    "print(\"FileCheckerActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Local Configuration File\n",
    "\n",
    "Create a local training configuration file that we'll later modify and sync to worker nodes. This simulates a common workflow where you want to tweak hyperparameters or training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local workspace directory for our custom config\n",
    "local_workspace = \"/teamspace/studios/this_studio/monarch_sync_example\"\n",
    "os.makedirs(local_workspace, exist_ok=True)\n",
    "\n",
    "# Create a custom training configuration file\n",
    "config_file_name = \"custom_training_config.toml\"\n",
    "local_config_path = os.path.join(local_workspace, config_file_name)\n",
    "\n",
    "# Write initial configuration\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(\"\"\"# TorchTitan Custom Training Configuration\n",
    "# This file demonstrates workspace synchronization\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "max_steps = 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created local config file: {local_config_path}\")\n",
    "with open(local_config_path, 'r') as f:\n",
    "    print(f\"\\nInitial configuration:\\n{f.read()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Workspace and Perform Initial Sync\n",
    "\n",
    "Create a Monarch `Workspace` object and perform the initial synchronization to all remote worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.tools.config.workspace import Workspace\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Workspace object pointing to our local directory\n",
    "workspace = Workspace(dirs=[Path(local_workspace)])\n",
    "\n",
    "print(f\"Workspace configured: {workspace.dirs}\")\n",
    "print(f\"\\nSyncing workspace to remote nodes...\")\n",
    "# Perform initial sync\n",
    "await proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n",
    "\n",
    "print(\"Initial workspace sync completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify File on Remote Nodes\n",
    "\n",
    "Check that the configuration file was successfully synced to all remote worker nodes by reading it from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the remote file path (files are synced to WORKSPACE_DIR)\n",
    "remote_workspace_root = os.environ.get(\"WORKSPACE_DIR\", \"/workspace\")\n",
    "remote_config_path = os.path.join(remote_workspace_root, \"monarch_sync_example\", config_file_name)\n",
    "\n",
    "print(f\"Checking file on remote nodes: {remote_config_path}\\n\")\n",
    "\n",
    "# Check file existence on all nodes\n",
    "exists_results = await file_checker.file_exists.call(remote_config_path)\n",
    "for result in exists_results:\n",
    "    status = \"EXISTS\" if result['exists'] else \" NOT FOUND\"\n",
    "    print(f\"  Rank {result['rank']} ({result['hostname']}): {status}\")\n",
    "\n",
    "# Read file content from rank 0 to verify\n",
    "print(f\"\\nReading config from rank 0:\")\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    print(f\"\\n{read_results[0]['content']}\")\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Debugging with Breakpoints in Monarch\n",
    "\n",
    "Monarch supports interactive debugging of distributed actors using Python's built-in `pdb` debugger. You can set breakpoints in your actors, attach to specific ranks, and inspect their state during execution.\n",
    "\n",
    "### How to Debug:\n",
    "\n",
    "1. **Add breakpoints** to your actor endpoints using `breakpoint()`\n",
    "2. **Run your training** as usual - execution will pause when breakpoints are hit\n",
    "3. **Open a separate terminal** and run: `monarch debug`\n",
    "4. **Use debugger commands**:\n",
    "   - `list` - Show all active breakpoints across ranks\n",
    "   - `attach <actor_name> <rank>` - Attach to a specific actor/rank for interactive debugging\n",
    "   - `cast <actor_name> ranks(<ranks>) <pdb_command>` - Send pdb commands to multiple ranks\n",
    "   - `continue` - Resume execution\n",
    "\n",
    "Let's create a debugging example using a TorchTitan trainer with breakpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define TitanTrainerActor with Breakpoints\n",
    "\n",
    "Create a TorchTitan trainer actor with breakpoints at key stages. This allows you to inspect the training state, configuration, and execution flow interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanTrainerDebug(Actor):\n",
    "    \"\"\"TorchTitan Trainer Actor with debugging breakpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "        self.trainer: Optional[Trainer] = None\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        self._rprint(f\"Initializing debug actor: {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "        # Breakpoint 1: After initialization\n",
    "        breakpoint()  # Debug: Inspect actor initialization state\n",
    "\n",
    "    @endpoint\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Setup the trainer with a breakpoint to inspect configuration.\"\"\"\n",
    "        logger.info(f\"Setting up trainer on rank {self.rank}\")\n",
    "        config = self.job_config\n",
    "\n",
    "        # Breakpoint 2: Before trainer creation\n",
    "        if self.rank == 0:  # Only break on rank 0 for simplicity\n",
    "            breakpoint()  # Debug: Inspect job config before trainer creation\n",
    "\n",
    "        self.trainer = Trainer(config)\n",
    "        self._rprint(\"Trainer setup complete\")\n",
    "\n",
    "    @endpoint\n",
    "    def train_step(self, num_steps: int = 5):\n",
    "        \"\"\"Run a few training steps with breakpoints.\"\"\"\n",
    "        if not self.trainer:\n",
    "            raise RuntimeError(\"Trainer not initialized. Call setup_trainer first.\")\n",
    "\n",
    "        logger.info(f\"Starting training for {num_steps} steps on rank {self.rank}\")\n",
    "\n",
    "        # Breakpoint 3: Before training starts\n",
    "        if self.rank == 0:\n",
    "            breakpoint()  # Debug: Inspect trainer state before training\n",
    "\n",
    "        # In a real scenario, you'd call trainer.train()\n",
    "        # For debugging purposes, we'll just simulate a few steps\n",
    "        for step in range(num_steps):\n",
    "            if step == 2 and self.rank == 0:  # Break mid-training on rank 0\n",
    "                breakpoint()  # Debug: Inspect mid-training state\n",
    "\n",
    "            self._rprint(f\"Processing step {step + 1}/{num_steps}\")\n",
    "\n",
    "        self._rprint(f\"Completed {num_steps} training steps\")\n",
    "\n",
    "    @endpoint\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources.\"\"\"\n",
    "        logger.info(f\"Cleaning up trainer on rank {self.rank}\")\n",
    "\n",
    "        if self.trainer:\n",
    "            self.trainer.close()\n",
    "\n",
    "        if torch.distributed.is_initialized():\n",
    "            torch.distributed.destroy_process_group()\n",
    "            logger.info(\"Process group destroyed.\")\n",
    "\n",
    "        self._rprint(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn Debug Trainer Actor\n",
    "\n",
    "Spawn the debug trainer actor across the process mesh. When you run the following cells, execution will pause at breakpoints, allowing you to debug interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the debug trainer actor\n",
    "debug_trainer = proc_mesh.spawn(\"debug_trainer\", TitanTrainerDebug, config)\n",
    "print(\"Debug trainer actor spawned across all nodes\")\n",
    "print(\"When breakpoints are hit, run 'monarch debug' in a separate terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Debug Training Session\n",
    "\n",
    "Execute the training endpoints. When breakpoints are hit:\n",
    "1. Open a separate terminal\n",
    "2. Run `monarch debug`\n",
    "3. Use `list` to see all active breakpoints\n",
    "4. Use `attach debug_trainer 0` to attach to rank 0\n",
    "5. Use standard pdb commands (`n`, `s`, `p <var>`, `l`, etc.)\n",
    "6. Use `continue` to resume execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize actors (will hit first breakpoint)\n",
    "await debug_trainer.init.call()\n",
    "\n",
    "# Setup trainer (will hit second breakpoint on rank 0)\n",
    "await debug_trainer.setup_trainer.call()\n",
    "\n",
    "# Run training steps (will hit breakpoints during training)\n",
    "await debug_trainer.train_step.call(num_steps=5)\n",
    "\n",
    "# Cleanup\n",
    "await debug_trainer.cleanup.call()\n",
    "\n",
    "print(\"Debug training session completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Debugger Commands\n",
    "\n",
    "Once in the Monarch debugger, try these commands:\n",
    "\n",
    "```bash\n",
    "# List all active breakpoints\n",
    "monarch_dbg> list\n",
    "\n",
    "# Attach to rank 0 for interactive debugging\n",
    "monarch_dbg> attach debug_trainer 0\n",
    "\n",
    "# Standard pdb commands when attached:\n",
    "(Pdb) n              # Next line\n",
    "(Pdb) s              # Step into function\n",
    "(Pdb) p self.rank    # Print variable\n",
    "(Pdb) l              # List source code\n",
    "(Pdb) c              # Continue execution\n",
    "\n",
    "# Cast commands to multiple ranks (without attaching)\n",
    "monarch_dbg> cast debug_trainer ranks(0,1) n\n",
    "monarch_dbg> cast debug_trainer ranks(0:4) c\n",
    "\n",
    "# Continue all breakpoints\n",
    "monarch_dbg> continue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup and Stop Process Mesh\n",
    "\n",
    "Gracefully stop the Monarch process mesh, cleaning up all distributed resources and shutting down the actors across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await proc_mesh.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
