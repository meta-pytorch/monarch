{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightning_sdk import Machine, MMT, Studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.192.12.177\n"
          ]
        }
      ],
      "source": [
        "from utils.master_node import MasterNodeServer\n",
        "private_master_host_ip_address = MasterNodeServer.get_master_ip()\n",
        "public_master_host_ip_address = MasterNodeServer.get_master_public_ip_curl()\n",
        "public_master_host_ip_address_services = MasterNodeServer.get_master_public_ip()\n",
        "print(f\"private_master_host_ip_address = {private_master_host_ip_address}\")\n",
        "print(f\"public_master_host_ip_address = {public_master_host_ip_address}\")\n",
        "print(f\"public_master_host_ip_address = {public_master_host_ip_address_services}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "NUM_NODES = 2\n",
        "NUM_GPUS = 8\n",
        "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
        "USER = \"meta-ai\"  # Replace with your username\n",
        "MONARCH_DEFAULT_PORT = 26600 # Monarch default port\n",
        "HTTP_SERVER_PORT = MONARCH_DEFAULT_PORT # 8080 # HTTP Server PORT for IP registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
        "    \"\"\"\n",
        "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
        "    \"\"\"\n",
        "\n",
        "    studio = Studio()\n",
        "\n",
        "    # Install the MMT plugin befor running the actual job\n",
        "    studio.install_plugin(\"multi-machine-training\")\n",
        "\n",
        "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
        "\n",
        "    # Machine with T4 GPUs\n",
        "    # machine_type = getattr(Machine, f\"T4_X_{NUM_GPUS}\")\n",
        "\n",
        "    # Machine with L40S GPUs\n",
        "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
        "\n",
        "    job = MMT.run(\n",
        "        command=f\"python example/utils/worker_node.py {public_master_host_ip_address} {HTTP_SERVER_PORT} && sleep 10 && process_allocator\",\n",
        "        name=\"Multi-Node-Monarch-Titan\",\n",
        "        # machine=Machine.T4_X_4,  # Use GPU machines for training\n",
        "        machine=machine_type,\n",
        "        studio=studio,\n",
        "        num_machines=num_nodes,\n",
        "        env={\n",
        "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible # TODO: Should make this one dynamic\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(f\"Job started with ID: {job.name}\")\n",
        "    print(f\"Job status: {job.status}\")\n",
        "\n",
        "    # Monitor job status\n",
        "    return job, studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching MMT job with 2 nodes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/Multi-Node-Monarch-Titan?app_id=mmt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job started with ID: Multi-Node-Monarch-Titan\n",
            "Job status: Pending\n",
            "Job launched. You can monitor it using: job.status\n",
            "To stop the job: job.stop()\n",
            "To clean up: studio.stop()\n"
          ]
        }
      ],
      "source": [
        "# Launch the job\n",
        "job, studio = launch_mmt_job(\n",
        "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
        ")\n",
        "\n",
        "print(f\"Job launched. You can monitor it using: job.status\")\n",
        "print(f\"To stop the job: job.stop()\")\n",
        "print(f\"To clean up: studio.stop()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master node IP: 10.192.12.177\n",
            "Expecting 2 worker nodes to register...\n",
            "Starting server on port 8080...\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 0s\n",
            "Server started on 10.192.12.177:8080\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 30s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 60s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 90s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 120s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 150s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 180s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 210s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 240s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 270s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 300s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 330s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 360s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 390s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 420s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 450s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 480s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 510s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 540s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 570s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 600s\n",
            "Waiting for workers... (0/2 registered) - Elapsed: 630s\n",
            "Registered worker node: 10.192.12.52 (1/2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10.192.12.52 - - [19/Sep/2025 03:51:42] \"POST /register HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered worker node: 10.192.12.72 (2/2)\n",
            "All worker nodes registered!\n",
            "Registration server stopped\n",
            "Final registered worker nodes: ['10.192.12.52', '10.192.12.72']\n",
            "Worker IPs saved to /tmp/worker_nodes.txt\n",
            "Cluster info saved to /tmp/cluster_info.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10.192.12.72 - - [19/Sep/2025 03:51:44] \"POST /register HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from utils.master_node import run_master_server\n",
        "cluster_info = run_master_server(expected_workers=NUM_NODES, port=HTTP_SERVER_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted IP addresses:\n",
            "10.192.12.52\n",
            "10.192.12.72\n",
            "\n",
            "IP set: {'10.192.12.72', '10.192.12.52'}\n",
            "['10.192.12.72', '10.192.12.52']\n"
          ]
        }
      ],
      "source": [
        "from utils.ip_utils import extract_ips_simple\n",
        "worker_nodes_ip_file_path = \"/tmp/worker_nodes.txt\"\n",
        "ip_addresses_set = extract_ips_simple(worker_nodes_ip_file_path)\n",
        "ip_addresses_list = list(ip_addresses_set)\n",
        "print(ip_addresses_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcp!10.192.12.72:26600 tcp!10.192.12.52:26600\n"
          ]
        }
      ],
      "source": [
        "tcp_addresses = [f\"tcp!{ip}:26600\" for ip in ip_addresses_set]\n",
        "\n",
        "# # Or if you want to test it locally first on the local machine uncomment line below:\n",
        "# tcp_addresses = [\"tcp![::]:26600\"]\n",
        "# # For the local host machine only, please make sure that NUM_NODES is equal to 1;\n",
        "# NUM_NODES = 1\n",
        "\n",
        "print(*tcp_addresses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 1 - Run TorchTitan using Monarch for Llama 3 - 8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from monarch._src.actor.allocator import RemoteAllocator, StaticRemoteAllocInitializer\n",
        "from monarch._rust_bindings.monarch_hyperactor.alloc import AllocConstraints, AllocSpec\n",
        "from monarch.actor import ProcMesh\n",
        "\n",
        "allocator = RemoteAllocator(\n",
        "        world_id=\"foo\",\n",
        "        initializer=StaticRemoteAllocInitializer(*tcp_addresses),\n",
        "    )\n",
        "\n",
        "alloc = allocator.allocate(\n",
        "        AllocSpec(AllocConstraints(), hosts=NUM_NODES, gpus=NUM_GPUS)\n",
        "    )\n",
        "\n",
        "proc_mesh = await ProcMesh.from_alloc(alloc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "monarch-alisol-hosts2-gpus8\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
        "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
        "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
        "import socket\n",
        "from torchtitan.tools.logging import init_logger, logger\n",
        "from torchtitan.train import Trainer\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torchtitan.config import JobConfig\n",
        "\n",
        "\n",
        "class TitanTrainerWrapper(Actor):\n",
        "    def __init__(self, job_config: JobConfig):\n",
        "        self.rank = current_rank().rank\n",
        "        self.job_config = job_config\n",
        "\n",
        "    def _rprint(self, msg):\n",
        "        \"\"\"Helper method to print with rank information.\"\"\"\n",
        "        print(f\"{self.rank=} {msg}\")\n",
        "\n",
        "    @endpoint\n",
        "    def init(self):\n",
        "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
        "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
        "\n",
        "\n",
        "    @endpoint\n",
        "    def train(self):\n",
        "        logger.info(\"Starting training\")\n",
        "        config = self.job_config\n",
        "        trainer: Optional[Trainer] = None\n",
        "\n",
        "        try:\n",
        "            trainer = Trainer(config)\n",
        "            trainer.train()\n",
        "\n",
        "            if config.checkpoint.create_seed_checkpoint:\n",
        "                assert (\n",
        "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
        "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
        "                assert (\n",
        "                    # config.checkpoint.enable_checkpoint\n",
        "                    config.checkpoint.enable\n",
        "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
        "                trainer.checkpointer.save(curr_step=0, )\n",
        "                logger.info(\"Created seed checkpoint\")\n",
        "            else:\n",
        "                trainer.train()\n",
        "        finally:\n",
        "            if trainer:\n",
        "                trainer.close()\n",
        "\n",
        "            if torch.distributed.is_initialized():\n",
        "                torch.distributed.destroy_process_group()\n",
        "                logger.info(\"Process group destroyed.\")\n",
        "        print(\"Done training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.xpu import stream\n",
        "from torchtitan.config import ConfigManager, JobConfig\n",
        "from monarch.utils import setup_env_for_distributed\n",
        "\n",
        "async def async_main(job_config: JobConfig):\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
        "\n",
        "    await setup_env_for_distributed(proc_mesh)\n",
        "\n",
        "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
        "\n",
        "    print(job_config)\n",
        "    print(f\"Spawning meshes on {job_name}\")\n",
        "\n",
        "    trainer_actor = await proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
        "    await trainer_actor.init.call()\n",
        "    await trainer_actor.train.call()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[titan] 2025-09-19 03:54:41,922 - root - WARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JobConfig(job=Job(config_file='/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml', dump_folder='/teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8', description='Llama 3 8B training', print_args=False), profiling=Profiling(enable_profiling=True, save_traces_folder='profile_trace', profile_freq=100, enable_memory_snapshot=False, save_memory_snapshot_folder='memory_snapshot'), metrics=Metrics(log_freq=1, enable_tensorboard=True, disable_color_printing=False, save_tb_folder='tb', save_for_all_ranks=False, enable_wandb=True), model=Model(name='llama3', flavor='8B', hf_assets_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', tokenizer_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', converters=[], print_after_conversion=False), optimizer=Optimizer(name='AdamW', lr=0.0003, beta1=0.9, beta2=0.95, eps=1e-08, weight_decay=0.1, implementation='fused', early_step_in_backward=False), lr_scheduler=LRScheduler(warmup_steps=200, decay_ratio=None, decay_type='linear', min_lr_factor=0.0), training=Training(dataset='c4_test', dataset_path='/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test', local_batch_size=1, global_batch_size=-1, seq_len=2048, max_norm=1.0, steps=25, enable_cpu_offload=False, mixed_precision_param='bfloat16', mixed_precision_reduce='float32', gc_freq=50, gc_debug=False, seed=None, deterministic=False), parallelism=Parallelism(data_parallel_replicate_degree=1, enable_compiled_autograd=False, data_parallel_shard_degree=-1, fsdp_reshard_after_forward='default', tensor_parallel_degree=1, disable_loss_parallel=False, enable_async_tensor_parallel=False, pipeline_parallel_degree=1, pipeline_parallel_split_points=[], module_fqns_per_model_part=None, pipeline_parallel_first_stage_less_layers=1, pipeline_parallel_last_stage_less_layers=1, pipeline_parallel_layers_per_stage=None, pipeline_parallel_schedule='1F1B', pipeline_parallel_schedule_csv='', pipeline_parallel_microbatch_size=1, context_parallel_degree=1, context_parallel_rotate_method='allgather', expert_parallel_degree=1, expert_tensor_parallel_degree=1), checkpoint=Checkpoint(enable=False, folder='checkpoint', interval=500, initial_load_path=None, initial_load_model_only=True, initial_load_in_hf=False, last_save_model_only=True, last_save_in_hf=False, export_dtype='float32', async_mode='disabled', keep_latest_k=10, load_step=-1, exclude_from_loading=[], enable_first_step_checkpoint=False, create_seed_checkpoint=False), activation_checkpoint=ActivationCheckpoint(mode='selective', selective_ac_option='op', per_op_sac_force_recompute_mm_shapes_by_fqns=['moe.router.gate'], early_stop=False), compile=Compile(enable=False, components=['model', 'loss']), float8=Float8(enable_fsdp_float8_all_gather=False, precompute_float8_dynamic_scale_for_fsdp=False, recipe_name=None, filter_fqns=['output'], emulate=False, moe_fqns_prototype=[]), mx=MX(mxfp8_dim1_cast_kernel_choice='triton', recipe_name='mxfp8_cublas', filter_fqns=['output'], moe_fqns_prototype=[]), comm=Comm(init_timeout_seconds=300, train_timeout_seconds=100, trace_buf_size=20000, save_traces_folder='comm_traces'), memory_estimation=MemoryEstimation(enable=False, disable_fake_mode=False), fault_tolerance=FaultTolerance(enable=False, process_group='gloo', process_group_timeout_ms=10000, replica_id=0, group_size=0, min_replica_size=1, semi_sync_method=None), experimental=Experimental(custom_import='', custom_args_module=''), validation=Validation(enable=False, dataset='c4_validation', dataset_path=None, local_batch_size=8, seq_len=2048, freq=500, steps=1200))\n",
            "Spawning meshes on monarch-alisol-hosts2-gpus8\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:11) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Initializing actor: 7 current_rank()={'hosts': 0/2, 'gpus': 7/8} socket.gethostname()='ip-10-192-12-72'\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:54:46) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:11) >>>\u001b[0m\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Starting training\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Starting job: Llama 3 8B training\n",
            "\u001b[33m[15 similar log lines]\u001b[0m [W919 03:54:48.025441173 socket.cpp:767] [c10d] The client socket has failed to connect to [ip-10-192-12-72]:50717 (errno: 22 - Invalid argument).\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Building 1-D device mesh with ['dp_shard'], [16]\n",
            "\u001b[33m[16 similar log lines]\u001b[0m [GC] Initial GC collection 0.00 seconds\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Loading tokenizer from tokenizer.json\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:54:49) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:46) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Initializing actor: 5 current_rank()={'hosts': 0/2, 'gpus': 5/8} socket.gethostname()='ip-10-192-12-72'\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:54:49) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:49) >>>\u001b[0m\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Preparing c4_test dataset from /teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)\n",
            "\u001b[33m[16 similar log lines]\u001b[0m CUDA capacity: NVIDIA L40S with 44.64GiB memory\n",
            "\u001b[33m[31 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
            "\u001b[33m[16 similar log lines]\u001b[0m \u001b[34mModel llama3 8B \u001b[31msize: 8,030,261,248 total parameters\u001b[39m\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Applied selective activation checkpointing to the model\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Applied FSDP to the model\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
            "\u001b[33m[15 similar log lines]\u001b[0m CUDA memory usage for model: 1.90GiB(4.25%)\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
            "\u001b[33m[15 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 25 (warmup 200)\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Training starts at step 1\n",
            "\u001b[33m[15 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/profile_trace\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Currently logged in as: a-shamsoshoara (a-shamsoshoara-m) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Tracking run with wandb version 0.21.3\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run data is saved locally in /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/tb/20250919-0354/wandb/run-20250919_035451-0p5lifho\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Syncing run graceful-river-27\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run at https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/0p5lifho\n",
            "\u001b[33m[1 similar log lines]\u001b[0m WandB logging enabled\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:54:52) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:52) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
            "\u001b[33m[1 similar log lines]\u001b[0m CUDA memory usage for model: 1.90GiB(4.25%)\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
            "\u001b[33m[1 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 25 (warmup 200)\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Training starts at step 1\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/profile_trace\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:54:55) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:55) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  1  \u001b[32mloss: 12.2286  \u001b[38;2;180;60;0mgrad_norm:  3.8083  \u001b[38;2;54;234;195mmemory: 17.41GiB(39.00%)  \u001b[34mtps: 98  \u001b[36mtflops: 4.72  \u001b[35mmfu: 1.51%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:11) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:11) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  1  \u001b[32mloss: 12.2286  \u001b[38;2;180;60;0mgrad_norm:  3.8083  \u001b[38;2;54;234;195mmemory: 17.41GiB(39.00%)  \u001b[34mtps: 98  \u001b[36mtflops: 4.72  \u001b[35mmfu: 1.51%\u001b[39m\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:14) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:14) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.4166  \u001b[38;2;180;60;0mgrad_norm:  5.0591  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 118  \u001b[36mtflops: 5.71  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:28) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:28) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.4166  \u001b[38;2;180;60;0mgrad_norm:  5.0591  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 118  \u001b[36mtflops: 5.71  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:31) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:31) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 12.0842  \u001b[38;2;180;60;0mgrad_norm: 56.4885  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:46) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:46) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 12.0842  \u001b[38;2;180;60;0mgrad_norm: 56.4885  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:55:49) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:55:49) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 13.0378  \u001b[38;2;180;60;0mgrad_norm: 49.4604  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:03) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:03) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 13.0378  \u001b[38;2;180;60;0mgrad_norm: 49.4604  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:06) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:06) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.8098  \u001b[38;2;180;60;0mgrad_norm:  8.7387  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:20) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:20) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.8098  \u001b[38;2;180;60;0mgrad_norm:  8.7387  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:23) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:23) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 11.7958  \u001b[38;2;180;60;0mgrad_norm: 25.1134  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:37) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:37) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 11.7958  \u001b[38;2;180;60;0mgrad_norm: 25.1134  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:40) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:40) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 11.4801  \u001b[38;2;180;60;0mgrad_norm:  8.4318  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:55) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:55) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 11.4801  \u001b[38;2;180;60;0mgrad_norm:  8.4318  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:56:58) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:56:58) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 10.5124  \u001b[38;2;180;60;0mgrad_norm: 10.8082  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:12) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:12) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 10.5124  \u001b[38;2;180;60;0mgrad_norm: 10.8082  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:15) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:15) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 10.4442  \u001b[38;2;180;60;0mgrad_norm: 20.3616  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:29) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:29) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 10.4442  \u001b[38;2;180;60;0mgrad_norm: 20.3616  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:32) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:32) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss:  9.7792  \u001b[38;2;180;60;0mgrad_norm:  7.5678  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:46) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:46) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss:  9.7792  \u001b[38;2;180;60;0mgrad_norm:  7.5678  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:57:49) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:57:49) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss:  9.1549  \u001b[38;2;180;60;0mgrad_norm:  4.6241  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:04) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:04) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss:  9.1549  \u001b[38;2;180;60;0mgrad_norm:  4.6241  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:07) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:07) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.3845  \u001b[38;2;180;60;0mgrad_norm: 32.4210  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:21) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:21) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.3845  \u001b[38;2;180;60;0mgrad_norm: 32.4210  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:24) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:24) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss: 10.4570  \u001b[38;2;180;60;0mgrad_norm: 40.4274  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:38) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:38) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss: 10.4570  \u001b[38;2;180;60;0mgrad_norm: 40.4274  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:41) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:41) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss: 10.1626  \u001b[38;2;180;60;0mgrad_norm: 43.6353  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:55) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:55) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss: 10.1626  \u001b[38;2;180;60;0mgrad_norm: 43.6353  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:58:58) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:58:58) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.8694  \u001b[38;2;180;60;0mgrad_norm: 15.3759  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:13) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:13) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.8694  \u001b[38;2;180;60;0mgrad_norm: 15.3759  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:16) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:16) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.5247  \u001b[38;2;180;60;0mgrad_norm:  4.8650  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:30) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:30) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.5247  \u001b[38;2;180;60;0mgrad_norm:  4.8650  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:33) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:33) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.6274  \u001b[38;2;180;60;0mgrad_norm: 14.6862  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:47) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:47) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.6274  \u001b[38;2;180;60;0mgrad_norm: 14.6862  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 03:59:50) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:59:50) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  8.2122  \u001b[38;2;180;60;0mgrad_norm:  4.1870  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:04) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:04) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  8.2122  \u001b[38;2;180;60;0mgrad_norm:  4.1870  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:07) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:07) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  8.0256  \u001b[38;2;180;60;0mgrad_norm:  5.2243  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:22) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:22) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  8.0256  \u001b[38;2;180;60;0mgrad_norm:  5.2243  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:25) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:25) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss:  8.0121  \u001b[38;2;180;60;0mgrad_norm: 10.4728  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:39) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:39) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss:  8.0121  \u001b[38;2;180;60;0mgrad_norm: 10.4728  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:42) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:42) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  9.3447  \u001b[38;2;180;60;0mgrad_norm:  9.1810  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:56) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:56) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  9.3447  \u001b[38;2;180;60;0mgrad_norm:  9.1810  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:00:59) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:00:59) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  8.0301  \u001b[38;2;180;60;0mgrad_norm:  8.9817  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:13) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:13) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  8.0301  \u001b[38;2;180;60;0mgrad_norm:  8.9817  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.73  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:16) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:16) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  8.0246  \u001b[38;2;180;60;0mgrad_norm:  4.4800  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:31) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:31) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  8.0246  \u001b[38;2;180;60;0mgrad_norm:  4.4800  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.72  \u001b[35mmfu: 1.83%\u001b[39m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:34) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:34) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  7.9291  \u001b[38;2;180;60;0mgrad_norm:  3.7897  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.75  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:48) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:48) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  7.9291  \u001b[38;2;180;60;0mgrad_norm:  3.7897  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.75  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[33m[2 similar log lines]\u001b[0m Dataset c4_test is being re-looped\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:01:51) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:01:51) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  7.8506  \u001b[38;2;180;60;0mgrad_norm:  2.9660  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:02:05) <<<\u001b[0m\n",
            "\n",
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:02:05) >>>\u001b[0m\n",
            "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  7.8506  \u001b[38;2;180;60;0mgrad_norm:  2.9660  \u001b[38;2;54;234;195mmemory: 19.37GiB(43.40%)  \u001b[34mtps: 119  \u001b[36mtflops: 5.74  \u001b[35mmfu: 1.84%\u001b[39m\n",
            "\u001b[33m[31 similar log lines]\u001b[0m Training completed\n",
            "\u001b[33m[2 similar log lines]\u001b[0m Sleeping 2 seconds for other ranks to complete\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Training starts at step 26\n",
            "\u001b[33m[16 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/profile_trace\n",
            "\u001b[33m[14 similar log lines]\u001b[0m Process group destroyed.\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:02:08) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 03:54:49) >>>\u001b[0m\n",
            "\u001b[33m[14 similar log lines]\u001b[0m Done training\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:02:08) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:02:08) >>>\u001b[0m\n",
            "\u001b[33m[1 similar log lines]\u001b[0m Training completed\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: updating run metadata\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:                                                                                \n",
            "\u001b[33m[3 similar log lines]\u001b[0m wandb: \n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run history:\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:                    grad_norm ‚ñÅ‚ñÅ‚ñà‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_max_loss ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[33m[4 similar log lines]\u001b[0m wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅÔøΩÔøΩ\n",
            "\u001b[33m[2 similar log lines]\u001b[0m wandb:                           +7 ...\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run summary:\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:                    grad_norm 2.96598\n",
            "\u001b[33m[2 similar log lines]\u001b[0m Process group destroyed.\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss 7.85056\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_max_loss 8.68094\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr 0.0003\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:         memory/max_active(%) 30.14646\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_active(GiB) 13.4567\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_reserved(%) 43.40058\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/max_reserved(GiB) 19.37305\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/num_alloc_retries 0\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb:              memory/num_ooms 0\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run graceful-river-27 at: https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/0p5lifho\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[33m[1 similar log lines]\u001b[0m wandb: Find logs at: ./torchtitan/outputs/monarch-alisol-hosts2-gpus8/tb/20250919-0354/wandb/run-20250919_035451-0p5lifho/logs\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:02:11) <<<\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m>>> Aggregated Logs (2025-09-19 04:02:08) >>>\u001b[0m\n",
            "\u001b[33m[2 similar log lines]\u001b[0m Done training\n",
            "\u001b[36m<<< Aggregated Logs (2025-09-19 04:02:11) <<<\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "init_logger()\n",
        "config_manager = ConfigManager()\n",
        "\n",
        "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
        "\n",
        "manual_args = [\n",
        "        \"--job.config_file\",\n",
        "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
        "        \"--model.tokenizer-path\",\n",
        "        # f\"{FUSE_DST}/Llama-3.1-8B\",\n",
        "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
        "        \"--training.steps\",\n",
        "        \"25\",\n",
        "        \"--training.dataset_path\",\n",
        "        # f\"{FUSE_DST}/c4\",\n",
        "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
        "        \"--job.dump_folder\",\n",
        "        # f\"{FUSE_DST}/outputs/\" + job_name,\n",
        "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name\n",
        "    ]\n",
        "config = config_manager.parse_args(manual_args)\n",
        "await async_main(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<monarch._src.actor.future.Future at 0x70673f3e6950>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "proc_mesh.stop()"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "6d5af34b-6e4e-48c6-82c4-ccf442e377c5",
    "isAdHoc": false,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
