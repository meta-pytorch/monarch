{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "private_master_host_ip_address = 10.192.12.204\n",
      "public_master_host_ip_address = 3.84.102.51\n",
      "public_master_host_ip_address = 3.84.102.51\n"
     ]
    }
   ],
   "source": [
    "from utils.master_node import MasterNodeServer\n",
    "private_master_host_ip_address = MasterNodeServer.get_master_ip()\n",
    "public_master_host_ip_address = MasterNodeServer.get_master_public_ip_curl()\n",
    "public_master_host_ip_address_services = MasterNodeServer.get_master_public_ip()\n",
    "print(f\"private_master_host_ip_address = {private_master_host_ip_address}\")\n",
    "print(f\"public_master_host_ip_address = {public_master_host_ip_address}\")\n",
    "print(f\"public_master_host_ip_address = {public_master_host_ip_address_services}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
    "USER = \"meta-ai\"  # Replace with your username\n",
    "MONARCH_DEFAULT_PORT = 26600 # Monarch default port\n",
    "HTTP_SERVER_PORT = MONARCH_DEFAULT_PORT # 8080 # HTTP Server PORT for IP registration\n",
    "\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"MONARCH_FILE_LOG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    # Install the MMT plugin befor running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{NUM_GPUS}\")\n",
    "\n",
    "     # Machine with L40 GPUs\n",
    "    # machine_type = getattr(Machine, f\"L4_X_{NUM_GPUS}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
    "\n",
    "    job = MMT.run(\n",
    "        command=f\"python example/utils/worker_node.py {public_master_host_ip_address} {HTTP_SERVER_PORT} && sleep 10 && process_allocator\",\n",
    "        name=f\"Multi-Node-Monarch-Titan-Scale-{NUM_NODES}_nodes-port_override\",\n",
    "        # machine=Machine.T4_X_4,  # Use GPU machines for training\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible # TODO: Should make this one dynamic\n",
    "            \"MONARCH_FILE_LOG\": \"debug\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\": \"26601-26610\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\": \"true\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    # Monitor job status\n",
    "    return job, studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching MMT job with 2 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/Multi-Node-Monarch-Titan-Scale-2_nodes-port_override-hspx9?app_id=mmt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: Multi-Node-Monarch-Titan-Scale-2_nodes-port_override-hspx9\n",
      "Job status: Pending\n",
      "Job launched. You can monitor it using: job.status\n",
      "To stop the job: job.stop()\n",
      "To clean up: studio.stop()\n"
     ]
    }
   ],
   "source": [
    "# Launch the job\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master node IP: 3.84.102.51\n",
      "Expecting 2 worker nodes to register...\n",
      "Starting server on port 26600...\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 0s\n",
      "Server started on 3.84.102.51:26600\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 30s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 60s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 90s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 120s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 150s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 180s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 210s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 240s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 270s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 300s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 330s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 360s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 390s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 420s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 450s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 480s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 510s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 540s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 570s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 600s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 630s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 660s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 690s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 720s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 750s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 780s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 810s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 840s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 870s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 900s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 930s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 960s\n",
      "Waiting for workers... (0/2 registered) - Elapsed: 990s\n",
      "Registered worker node: 52.14.215.20 (1/2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52.14.215.20 - - [14/Oct/2025 00:41:36] \"POST /register HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered worker node: 18.219.107.185 (2/2)\n",
      "All worker nodes registered!\n",
      "Registration server stopped\n",
      "Final registered worker nodes: ['52.14.215.20', '18.219.107.185']\n",
      "Worker IPs saved to /tmp/worker_nodes.txt\n",
      "Cluster info saved to /tmp/cluster_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18.219.107.185 - - [14/Oct/2025 00:41:40] \"POST /register HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from utils.master_node import run_master_server\n",
    "cluster_info = run_master_server(expected_workers=NUM_NODES, port=HTTP_SERVER_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted IP addresses:\n",
      "18.219.107.185\n",
      "52.14.215.20\n",
      "\n",
      "IP set: {'18.219.107.185', '52.14.215.20'}\n",
      "['18.219.107.185', '52.14.215.20']\n"
     ]
    }
   ],
   "source": [
    "from utils.ip_utils import extract_ips_simple\n",
    "worker_nodes_ip_file_path = \"/tmp/worker_nodes.txt\"\n",
    "ip_addresses_set = extract_ips_simple(worker_nodes_ip_file_path)\n",
    "ip_addresses_list = list(ip_addresses_set)\n",
    "print(ip_addresses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp!18.219.107.185:26600 tcp!52.14.215.20:26600\n"
     ]
    }
   ],
   "source": [
    "# ip_addresses_set = {'3.143.199.198', '3.132.52.102', '3.15.95.43'}\n",
    "# ip_addresses_set = {'18.219.107.185', '52.14.215.20'}\n",
    "tcp_addresses = [f\"tcp!{ip}:{MONARCH_DEFAULT_PORT}\" for ip in ip_addresses_set]\n",
    "\n",
    "# # Or if you want to test it locally first on the local machine uncomment line below:\n",
    "# tcp_addresses = [\"tcp![::]:26600\"]\n",
    "# # For the local host machine only, please make sure that NUM_NODES is equal to 1;\n",
    "# NUM_NODES = 1\n",
    "\n",
    "print(*tcp_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Run TorchTitan using Monarch for Llama 3 - 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllocHandle(_hy_alloc=<monarch._rust_bindings.monarch_hyperactor.pytokio.Shared object at 0x70ec091c9f20>, _extent={'hosts': 2, 'gpus': 8}, _stream_logs=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: UserWarning: The AllocSpec passed to RemoteAllocator.allocate has transport unix, but the transport from the remote process alloc initializer is tcp. This will soon be an error unless you explicitly configure monarch's default transport to tcp. The current default transport is unix.\n"
     ]
    }
   ],
   "source": [
    "from monarch._src.actor.allocator import RemoteAllocator, StaticRemoteAllocInitializer\n",
    "from monarch._rust_bindings.monarch_hyperactor.alloc import AllocConstraints, AllocSpec\n",
    "from monarch.actor import ProcMesh\n",
    "import os\n",
    "\n",
    "# os.environ[\"HYPERACTOR_REMOTE_PROCESS_ALLOC_PORT\"] = \"26600\"\n",
    "# os.environ[\"HYPERACTOR_REMOTE_PROCESS_ALLOC_ADDR\"] = f\"tcp!{public_master_host_ip_address}:{MONARCH_DEFAULT_PORT}\"\n",
    "# os.environ[\"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\"] = \"26600-26610\"\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\"] = \"26600-26610\"\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"] = f\"tcp!{public_master_host_ip_address}:0\"\n",
    "# os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"] = \"tcp!127.0.0.1:0\"\n",
    "# os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"] = \"tcp!3.84.102.51:0\"\n",
    "# os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"] = \"tcp!10.192.12.204:0\"\n",
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\"] = \"true\"\n",
    "\n",
    "\n",
    "allocator = RemoteAllocator(\n",
    "        world_id=\"foo\",\n",
    "        initializer=StaticRemoteAllocInitializer(*tcp_addresses),\n",
    "    )\n",
    "\n",
    "alloc = allocator.allocate(\n",
    "        AllocSpec(AllocConstraints(), hosts=NUM_NODES, gpus=NUM_GPUS)\n",
    "    )\n",
    "\n",
    "print(alloc)\n",
    "# proc_mesh = await ProcMesh.from_alloc(alloc)\n",
    "# proc_mesh = ProcMesh.from_alloc(alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_mesh = ProcMesh.from_alloc(alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tcp!3.84.102.51:0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"HYPERACTOR_REMOTE_ALLOC_BOOTSTRAP_ADDR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch-alisol-hosts2-gpus8\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert (\n",
    "                    # config.checkpoint.enable_checkpoint\n",
    "                    config.checkpoint.enable\n",
    "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0, )\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.xpu import stream\n",
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    await setup_env_for_distributed(proc_mesh,\n",
    "                                    )\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    # trainer_actor = await proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[titan] 2025-10-14 01:07:48,206 - root - WARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.\n",
      "JobConfig(job=Job(config_file='/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml', dump_folder='/teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8', description='Llama 3 8B training', print_args=False), profiling=Profiling(enable_profiling=True, save_traces_folder='profile_trace', profile_freq=100, profiler_active=1, profiler_warmup=3, enable_memory_snapshot=False, save_memory_snapshot_folder='memory_snapshot'), metrics=Metrics(log_freq=1, enable_tensorboard=True, disable_color_printing=False, save_tb_folder='tb', save_for_all_ranks=False, enable_wandb=True), model=Model(name='llama3', flavor='8B', hf_assets_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', tokenizer_path='/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B', converters=[], print_after_conversion=False), optimizer=Optimizer(name='AdamW', lr=0.0003, beta1=0.9, beta2=0.95, eps=1e-08, weight_decay=0.1, implementation='fused', early_step_in_backward=False), lr_scheduler=LRScheduler(warmup_steps=200, decay_ratio=None, decay_type='linear', min_lr_factor=0.0), training=Training(dataset='c4_test', dataset_path='/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test', local_batch_size=1, global_batch_size=-1, seq_len=1024, max_norm=1.0, steps=25, enable_cpu_offload=False, dtype='float32', mixed_precision_param='bfloat16', mixed_precision_reduce='float32', gc_freq=50, gc_debug=False, seed=None, deterministic=False, debug_moe_force_load_balance=False), parallelism=Parallelism(data_parallel_replicate_degree=1, enable_compiled_autograd=False, data_parallel_shard_degree=-1, fsdp_reshard_after_forward='default', tensor_parallel_degree=1, disable_loss_parallel=False, enable_async_tensor_parallel=False, pipeline_parallel_degree=1, pipeline_parallel_split_points=[], module_fqns_per_model_part=None, pipeline_parallel_first_stage_less_layers=1, pipeline_parallel_last_stage_less_layers=1, pipeline_parallel_layers_per_stage=None, pipeline_parallel_schedule='1F1B', pipeline_parallel_schedule_csv='', pipeline_parallel_microbatch_size=1, context_parallel_degree=1, context_parallel_rotate_method='allgather', expert_parallel_degree=1, expert_tensor_parallel_degree=1), checkpoint=Checkpoint(enable=False, folder='checkpoint', interval=500, initial_load_path=None, initial_load_model_only=True, initial_load_in_hf=False, initial_load_in_hf_quantized=False, last_save_model_only=True, last_save_in_hf=False, export_dtype='float32', async_mode='disabled', keep_latest_k=10, load_step=-1, exclude_from_loading=[], enable_first_step_checkpoint=False, create_seed_checkpoint=False, load_only=False), activation_checkpoint=ActivationCheckpoint(mode='selective', selective_ac_option='op', per_op_sac_force_recompute_mm_shapes_by_fqns=['moe.router.gate'], early_stop=False, memory_budget=0.5, visualize_memory_budget_pareto=False), compile=Compile(enable=False, components=['model', 'loss'], backend='inductor'), quantize=Quantize(linear=QuantizedLinear(float8=Float8Linear(enable_fsdp_float8_all_gather=False, precompute_float8_dynamic_scale_for_fsdp=False, recipe_name=None, filter_fqns=['output'], emulate=False), mx=MXLinear(mxfp8_dim1_cast_kernel_choice='triton', recipe_name='mxfp8_cublas', filter_fqns=['output'])), grouped_mm=QuantizedGroupedMM(float8=Float8GroupedMM(fqns=[]), mx=MXGroupedMM(recipe_name='mxfp8', fqns=[]))), comm=Comm(init_timeout_seconds=300, train_timeout_seconds=100, trace_buf_size=20000, save_traces_folder='comm_traces', save_traces_file_prefix='rank_'), memory_estimation=MemoryEstimation(enable=False, disable_fake_mode=False), fault_tolerance=FaultTolerance(enable=False, process_group='gloo', process_group_timeout_ms=10000, replica_id=0, group_size=0, min_replica_size=1, semi_sync_method=None), experimental=Experimental(custom_import='', custom_args_module=''), validation=Validation(enable=False, dataset='c4_validation', dataset_path=None, local_batch_size=8, seq_len=2048, freq=500, steps=1200))\n",
      "Spawning meshes on monarch-alisol-hosts2-gpus8\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:06:30) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Initializing actor: 3 current_rank()={'hosts': 0/2, 'gpus': 3/8} socket.gethostname()='ip-10-192-12-106'\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:07:52) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:06:30) >>>\u001b[0m\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Starting training\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Starting job: Llama 3 8B training\n",
      "\u001b[33m[5 similar log lines]\u001b[0m [W1014 01:07:53.636464501 socket.cpp:767] [c10d] The client socket has failed to connect to [ip-10-192-12-106]:50173 (errno: 22 - Invalid argument).\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Building 1-D device mesh with ['dp_shard'], [16]\n",
      "\u001b[33m[16 similar log lines]\u001b[0m [GC] Initial GC collection took 0.00 seconds\n",
      "\u001b[33m[11 similar log lines]\u001b[0m Loading tokenizer from tokenizer.json\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:07:55) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:07:52) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m Initializing actor: 9 current_rank()={'hosts': 1/2, 'gpus': 1/8} socket.gethostname()='ip-10-192-12-142'\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:07:55) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:07:55) >>>\u001b[0m\n",
      "\u001b[33m[5 similar log lines]\u001b[0m Loading tokenizer from tokenizer.json\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Preparing c4_test dataset from /teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, rope_scaling_args=RoPEScalingArgs(scaling_factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192), max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)\n",
      "\u001b[33m[16 similar log lines]\u001b[0m CUDA capacity: NVIDIA L40S with 44.64GiB memory\n",
      "\u001b[33m[32 similar log lines]\u001b[0m Peak flops undefined for: NVIDIA L40S, fallback to A100\n",
      "\u001b[33m[16 similar log lines]\u001b[0m \u001b[34mModel llama3 8B \u001b[31msize: 8,030,261,248 total parameters\u001b[39m\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Applied selective activation checkpointing to the model\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Applied FSDP to the model\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Peak FLOPS used for computing MFU: 3.120e+14\n",
      "\u001b[33m[16 similar log lines]\u001b[0m CUDA memory usage for model: 1.90GiB(4.25%)\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Warmup steps (200) exceed total training steps (25). Adjusting warmup steps to 25.\n",
      "\u001b[33m[16 similar log lines]\u001b[0m model.safetensors.index.json not found at hf_assets_path: /teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Mixed precision training is handled by fully_shard\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Trainer is initialized with local batch size 1, global batch size 16, gradient accumulation steps 1, sequence length 1024, total steps 25 (warmup 200)\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Training starts at step 1\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/profile_trace\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Currently logged in as: a-shamsoshoara (a-shamsoshoara-m) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Tracking run with wandb version 0.22.2\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run data is saved locally in /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/tb/20251014-0107/wandb/run-20251014_010756-napbrhpr\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Syncing run magic-fire-38\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run at https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/napbrhpr\n",
      "\u001b[33m[1 similar log lines]\u001b[0m WandB logging enabled\n",
      "\u001b[33m[1 similar log lines]\u001b[0m TensorBoard logging enabled. Logs will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/tb/20251014-0107\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:07:58) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:07:58) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  1  \u001b[32mloss: 12.2378  \u001b[38;2;180;60;0mgrad_norm:  4.0878  \u001b[38;2;54;234;195mmemory: 16.50GiB(36.97%)  \u001b[34mtps: 51  \u001b[36mtflops: 2.37  \u001b[35mmfu: 0.76%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:15) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:15) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  1  \u001b[32mloss: 12.2378  \u001b[38;2;180;60;0mgrad_norm:  4.0878  \u001b[38;2;54;234;195mmemory: 16.50GiB(36.97%)  \u001b[34mtps: 51  \u001b[36mtflops: 2.36  \u001b[35mmfu: 0.76%\u001b[39m\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:18) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:18) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.5027  \u001b[38;2;180;60;0mgrad_norm:  4.2669  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:33) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:33) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  2  \u001b[32mloss: 11.5027  \u001b[38;2;180;60;0mgrad_norm:  4.2669  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:36) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:36) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 11.1583  \u001b[38;2;180;60;0mgrad_norm: 24.6423  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:50) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:50) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  3  \u001b[32mloss: 11.1583  \u001b[38;2;180;60;0mgrad_norm: 24.6423  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:08:53) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:08:53) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 11.9682  \u001b[38;2;180;60;0mgrad_norm: 30.1718  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:07) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:07) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  4  \u001b[32mloss: 11.9682  \u001b[38;2;180;60;0mgrad_norm: 30.1718  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:10) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:10) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.6185  \u001b[38;2;180;60;0mgrad_norm:  9.5712  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:25) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:25) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  5  \u001b[32mloss: 11.6185  \u001b[38;2;180;60;0mgrad_norm:  9.5712  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:28) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:28) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 12.5239  \u001b[38;2;180;60;0mgrad_norm: 35.6288  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:42) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:42) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  6  \u001b[32mloss: 12.5239  \u001b[38;2;180;60;0mgrad_norm: 35.6288  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:45) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:45) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 11.7442  \u001b[38;2;180;60;0mgrad_norm: 29.6369  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:09:59) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:09:59) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  7  \u001b[32mloss: 11.7442  \u001b[38;2;180;60;0mgrad_norm: 29.6369  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:02) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:02) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 11.1135  \u001b[38;2;180;60;0mgrad_norm:  7.6529  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:17) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:17) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  8  \u001b[32mloss: 11.1135  \u001b[38;2;180;60;0mgrad_norm:  7.6529  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:20) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:20) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 10.2921  \u001b[38;2;180;60;0mgrad_norm: 10.3646  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:34) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:34) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep:  9  \u001b[32mloss: 10.2921  \u001b[38;2;180;60;0mgrad_norm: 10.3646  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:37) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:37) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss:  9.8799  \u001b[38;2;180;60;0mgrad_norm: 11.4258  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:51) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:51) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 10  \u001b[32mloss:  9.8799  \u001b[38;2;180;60;0mgrad_norm: 11.4258  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:10:54) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:10:54) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss:  9.4768  \u001b[38;2;180;60;0mgrad_norm:  6.1265  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.89%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:09) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:09) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 11  \u001b[32mloss:  9.4768  \u001b[38;2;180;60;0mgrad_norm:  6.1265  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.89%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:12) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:12) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.6915  \u001b[38;2;180;60;0mgrad_norm: 23.5042  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:26) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:26) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 12  \u001b[32mloss:  9.6915  \u001b[38;2;180;60;0mgrad_norm: 23.5042  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:29) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:29) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss:  8.9296  \u001b[38;2;180;60;0mgrad_norm: 11.6299  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:43) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:43) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 13  \u001b[32mloss:  8.9296  \u001b[38;2;180;60;0mgrad_norm: 11.6299  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:11:46) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:11:46) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss:  8.6181  \u001b[38;2;180;60;0mgrad_norm:  6.6545  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:01) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:01) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 14  \u001b[32mloss:  8.6181  \u001b[38;2;180;60;0mgrad_norm:  6.6545  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:04) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:04) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.8019  \u001b[38;2;180;60;0mgrad_norm: 13.4164  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:18) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:18) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 15  \u001b[32mloss:  8.8019  \u001b[38;2;180;60;0mgrad_norm: 13.4164  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:21) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:21) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.5115  \u001b[38;2;180;60;0mgrad_norm:  6.6547  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:35) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:35) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 16  \u001b[32mloss:  8.5115  \u001b[38;2;180;60;0mgrad_norm:  6.6547  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:38) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:38) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.4151  \u001b[38;2;180;60;0mgrad_norm:  9.0338  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:53) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:53) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 17  \u001b[32mloss:  8.4151  \u001b[38;2;180;60;0mgrad_norm:  9.0338  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:12:56) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:12:56) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  8.1653  \u001b[38;2;180;60;0mgrad_norm:  4.2390  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:10) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:10) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 18  \u001b[32mloss:  8.1653  \u001b[38;2;180;60;0mgrad_norm:  4.2390  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:13) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:13) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  8.0662  \u001b[38;2;180;60;0mgrad_norm:  8.6250  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:27) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:27) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 19  \u001b[32mloss:  8.0662  \u001b[38;2;180;60;0mgrad_norm:  8.6250  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:30) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:30) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss: 13.0456  \u001b[38;2;180;60;0mgrad_norm: 556.3015  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:45) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:45) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 20  \u001b[32mloss: 13.0456  \u001b[38;2;180;60;0mgrad_norm: 556.3015  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:13:48) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:13:48) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  8.4873  \u001b[38;2;180;60;0mgrad_norm: 16.7799  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:02) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:02) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 21  \u001b[32mloss:  8.4873  \u001b[38;2;180;60;0mgrad_norm: 16.7799  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:05) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:05) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  8.1071  \u001b[38;2;180;60;0mgrad_norm:  5.1292  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:20) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:20) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 22  \u001b[32mloss:  8.1071  \u001b[38;2;180;60;0mgrad_norm:  5.1292  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:23) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:23) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  8.0859  \u001b[38;2;180;60;0mgrad_norm: 28.9328  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:37) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:37) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 23  \u001b[32mloss:  8.0859  \u001b[38;2;180;60;0mgrad_norm: 28.9328  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.75  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:40) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:40) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  8.2353  \u001b[38;2;180;60;0mgrad_norm:  6.1007  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.77  \u001b[35mmfu: 0.89%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:54) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:54) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 24  \u001b[32mloss:  8.2353  \u001b[38;2;180;60;0mgrad_norm:  6.1007  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.77  \u001b[35mmfu: 0.89%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:14:57) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:14:57) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  8.3818  \u001b[38;2;180;60;0mgrad_norm:  3.8415  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:15:11) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:07:55) >>>\u001b[0m\n",
      "\u001b[33m[14 similar log lines]\u001b[0m Done training\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:15:14) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:15:11) >>>\u001b[0m\n",
      "\u001b[33m[15 similar log lines]\u001b[0m \u001b[31mstep: 25  \u001b[32mloss:  8.3818  \u001b[38;2;180;60;0mgrad_norm:  3.8415  \u001b[38;2;54;234;195mmemory: 18.47GiB(41.37%)  \u001b[34mtps: 59  \u001b[36mtflops: 2.76  \u001b[35mmfu: 0.88%\u001b[39m\n",
      "\u001b[33m[31 similar log lines]\u001b[0m Training completed\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Training starts at step 26\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Sleeping 2 seconds for other ranks to complete\n",
      "\u001b[33m[16 similar log lines]\u001b[0m Profiling active. Traces will be saved at /teamspace/studios/this_studio/torchtitan/outputs/monarch-alisol-hosts2-gpus8/profile_trace\n",
      "\u001b[33m[14 similar log lines]\u001b[0m Process group destroyed.\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:15:14) <<<\u001b[0m\n",
      "\n",
      "\u001b[36m>>> Aggregated Logs ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m>>> Aggregated Logs (2025-10-14 01:15:14) >>>\u001b[0m\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Done training\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:15:17) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 01:15:14) >>>\u001b[0m\n",
      "\u001b[33m[1 similar log lines]\u001b[0m Training completed\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: updating run metadata\n",
      "\u001b[33m[3 similar log lines]\u001b[0m wandb: \n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run history:\n",
      "\u001b[33m[2 similar log lines]\u001b[0m wandb:                    grad_norm ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_max_loss ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[33m[4 similar log lines]\u001b[0m wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[33m[2 similar log lines]\u001b[0m wandb:                           +7 ...\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Run summary:\n",
      "\u001b[33m[2 similar log lines]\u001b[0m Process group destroyed.\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                    grad_norm 3.84154\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_avg_loss 8.38181\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: loss_metrics/global_max_loss 10.08879\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:                           lr 0.0003\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:         memory/max_active(%) 24.73506\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_active(GiB) 11.04117\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:       memory/max_reserved(%) 41.37035\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/max_reserved(GiB) 18.4668\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:     memory/num_alloc_retries 0\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb:              memory/num_ooms 0\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: üöÄ View run magic-fire-38 at: https://wandb.ai/a-shamsoshoara-m/torchtitan/runs/napbrhpr\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/a-shamsoshoara-m/torchtitan\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[33m[1 similar log lines]\u001b[0m wandb: Find logs at: ./torchtitan/outputs/monarch-alisol-hosts2-gpus8/tb/20251014-0107/wandb/run-20251014_010756-napbrhpr/logs\n",
      "\u001b[36m<<< Aggregated Logs (2025-10-14 01:15:17) <<<\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "        \"--model.tokenizer-path\",\n",
    "        # f\"{FUSE_DST}/Llama-3.1-8B\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "        \"--training.steps\",\n",
    "        \"25\",\n",
    "        \"--training.dataset_path\",\n",
    "        # f\"{FUSE_DST}/c4\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "        \"--job.dump_folder\",\n",
    "        # f\"{FUSE_DST}/outputs/\" + job_name,\n",
    "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "        \"--training.seq_len\",\n",
    "        \"1024\",\n",
    "        # \"8192\",\n",
    "    ]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(TitanTrainerWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await proc_mesh.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
