{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studio 1: Getting Started - Multi-Node Training with Monarch & Lightning\n",
    "\n",
    "Welcome! This notebook will guide you through running **distributed multi-node training** using **Monarch** (Meta's distributed actor framework) with **TorchTitan** (PyTorch's large-scale LLM training library) on **Lightning AI** infrastructure.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/NB_Monarch_Lightning.svg\" alt=\"Monarch Lightning Architecture\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- Set up TorchTitan, Monarch, and Lightning SDK\n",
    "- Launch a multi-node training job on Lightning AI\n",
    "- Run distributed Llama-3-8B training across multiple GPUs\n",
    "- Monitor and manage your distributed training\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Lightning AI account with access to GPU machines (L40S recommended)\n",
    "- Hugging Face account with Llama model access\n",
    "- Basic understanding of distributed training concepts\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After completing this studio:\n",
    "- **Studio 2**: Learn workspace synchronization for hot-reloading configs without restarting jobs\n",
    "- **Studio 3**: Master interactive debugging across distributed nodes\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part I: Environment Setup\n",
    "\n",
    "Before running distributed training, we need to install dependencies. Follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TorchTitan\n",
    "\n",
    "Clone the TorchTitan repository, install the nightly PyTorch build with CUDA 12.6 support, and install TorchTitan:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/pytorch/torchtitan.git\n",
    "cd torchtitan\n",
    "pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Llama-3-8B Model Assets\n",
    "\n",
    "Download the Llama-3.1-8B tokenizer from Hugging Face. You'll need a Hugging Face token with access to the Llama models:\n",
    "\n",
    "```bash\n",
    "python scripts/download_hf_assets.py \\\n",
    "    --repo_id meta-llama/Llama-3.1-8B \\\n",
    "    --assets tokenizer \\\n",
    "    --hf_token=YOUR_HUGGINGFACE_TOKEN_KEY\n",
    "```\n",
    "\n",
    "Replace `YOUR_HUGGINGFACE_TOKEN_KEY` with your actual Hugging Face token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Monarch\n",
    "\n",
    "Install Monarch from the GitHub repository following the Ubuntu installation instructions:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/meta-pytorch/monarch.git\n",
    "cd monarch\n",
    "# Follow the Ubuntu installation instructions from the repository\n",
    "```\n",
    "\n",
    "For detailed installation steps, visit: https://github.com/meta-pytorch/monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weights & Biases\n",
    "\n",
    "Install wandb for experiment tracking:\n",
    "\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n",
    "\n",
    "Follow the prompts to authenticate with your wandb account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Lightning SDK\n",
    "\n",
    "Install the latest version of Lightning SDK for IP sharing features:\n",
    "\n",
    "```bash\n",
    "pip install -U lightning_sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installations\n",
    "\n",
    "After completing the installation steps above, verify that TorchTitan and Monarch are properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TorchTitan installation\n",
    "import torchtitan\n",
    "print(\"TorchTitan is installed successfully\")\n",
    "\n",
    "# Verify Monarch installation\n",
    "import monarch\n",
    "print(\"Monarch is installed successfully\")\n",
    "\n",
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Multi-Node Training with Monarch and Lightning\n",
    "\n",
    "Now that the environment is set up, we'll configure and launch distributed training across multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lightning SDK Components\n",
    "\n",
    "Import the necessary classes from Lightning SDK to manage multi-machine training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Job Parameters\n",
    "\n",
    "Set up the configuration for the multi-node training job. We'll start with **2 nodes** to keep things manageable.\n",
    "\n",
    "> **Note:** You can easily scale this up to 16+ nodes once you're comfortable with the workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
    "USER = \"your-username\"  # Replace with your username\n",
    "MMT_JOB_NAME = f\"Monarch-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "# Remote allowed port range for worker nodes\n",
    "REMOTE_ALLOWED_PORT_RANGE = \"26601..26611\"\n",
    "\n",
    "# To force Monarch to use V0 for this Notebook (This will be removed in the future)\n",
    "os.environ[\"MONARCH_V0_WORKAROUND_DO_NOT_USE\"] = \"1\"\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MMT Job Launch Function\n",
    "\n",
    "Create a function to launch a multi-machine training (MMT) job using Lightning SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    # Install the MMT plugin before running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
    "\n",
    "    job = MMT.run(\n",
    "        command=\"process_allocator\",\n",
    "        name=MMT_JOB_NAME,\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",\n",
    "            \"MONARCH_FILE_LOG\": \"debug\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\": REMOTE_ALLOWED_PORT_RANGE,\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\": \"true\",\n",
    "            \"WORKSPACE_DIR\": \"/tmp\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    return job, studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Multi-Node Training Job\n",
    "\n",
    "Execute the launch function to start the distributed training infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the job\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
    ")\n",
    "\n",
    "print(f\"\\nJob launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "You can monitor your job through the MMT plugin in Lightning AI. The nodes will go through these stages:\n",
    "\n",
    "1. **Pending** - Waiting for resources\n",
    "2. **Setting up** - Installing dependencies and snapshotting environment\n",
    "3. **Ready** - All nodes ready with `process_allocator` running\n",
    "\n",
    "Wait for all nodes to show **Ready** status before proceeding to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status\n",
    "print(f\"Current job status: {job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Process Mesh from Job\n",
    "\n",
    "Initialize the Monarch process mesh using the launched Lightning job. This creates the distributed computing mesh that connects all nodes and GPUs.\n",
    "\n",
    "> **Important:** Make sure the `process_allocator` process is running on all nodes before running this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mesh_utils import setup_proc_mesh_from_job\n",
    "\n",
    "proc_mesh = setup_proc_mesh_from_job(job, NUM_NODES, NUM_GPUS)\n",
    "print(\"\\nProcess mesh initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Run TorchTitan Training for Llama-3-8B\n",
    "\n",
    "Now we'll define a Monarch Actor that wraps TorchTitan's training functionality and run distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Job Name Helper\n",
    "\n",
    "Create a unique job name for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TorchTitan Trainer Actor\n",
    "\n",
    "Create the `TitanTrainerWrapper` class, a Monarch Actor that wraps TorchTitan's training functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert config.checkpoint.enable, \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0)\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Async Main Training Function\n",
    "\n",
    "Set up the main asynchronous function that orchestrates distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.tools.network import AddrType\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    # Use IPv4 for MASTER_ADDR\n",
    "    await setup_env_for_distributed(proc_mesh, use_ipaddr=AddrType.IPv4)\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Logger and Run Training\n",
    "\n",
    "Configure the TorchTitan logger, set up training parameters, and execute the training pipeline.\n",
    "\n",
    "> **Note:** This will train Llama-3-8B for 25 steps. Adjust the paths below to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "    \"--job.config_file\",\n",
    "    os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "    \"--model.tokenizer-path\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "    \"--training.steps\",\n",
    "    \"25\",\n",
    "    \"--training.dataset_path\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "    \"--job.dump_folder\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "    \"--training.seq_len\",\n",
    "    \"1024\",\n",
    "]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Congratulations! üéâ\n",
    "\n",
    "You just ran **interactive distributed training** for a Llama-3-8B model in a Jupyter notebook using **Monarch actors** and **Lightning infrastructure**!\n",
    "\n",
    "## What You Accomplished\n",
    "\n",
    "- Launched a multi-node training job on Lightning AI\n",
    "- Set up a distributed process mesh with Monarch\n",
    "- Ran TorchTitan training across multiple GPUs and nodes\n",
    "- Monitored training with aggregated logging\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Flexibility**: Change configurations and relaunch training without restarting nodes\n",
    "- **Observability**: Monarch aggregates logs from all ranks\n",
    "- **Scalability**: Easily scale from 2 to 16+ nodes by changing `NUM_NODES`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered the basics, explore advanced Monarch features:\n",
    "\n",
    "### üìö Studio 2: Workspace Synchronization (Recommended Next)\n",
    "Learn how to:\n",
    "- Sync local code/config changes to remote nodes **without restarting**\n",
    "- Hot-reload training configurations\n",
    "- Iterate faster on distributed training\n",
    "\n",
    "### üêõ Studio 3: Interactive Debugging\n",
    "Master advanced debugging:\n",
    "- Set breakpoints in distributed actors\n",
    "- Debug specific ranks interactively\n",
    "- Inspect environment variables across nodes\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "When you're done, remember to stop the process mesh and clean up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the process mesh\n",
    "await proc_mesh.stop()\n",
    "\n",
    "# Stop the Lightning job\n",
    "job.stop()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
