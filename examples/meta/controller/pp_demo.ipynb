{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env PYTHONBREAKPOINT=ipdb.set_trace\n",
    "import torch\n",
    "import torch._inductor.metrics as metrics\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# import torch\n",
    "from controller import (\n",
    "    DeviceMesh,\n",
    "    active_mesh,\n",
    "    active_stream,\n",
    "    Stream,\n",
    "    fetch_shard,\n",
    "    Future,\n",
    "    Stream,\n",
    "    get_active_stream,\n",
    ")\n",
    "from torch.fx.experimental.proxy_tensor import make_fx\n",
    "from controller._testing import simulator_mesh, example_mesh\n",
    "from controller.simulator import set_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch import explicit_autograd\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "def f(x):\n",
    "    return x.cos().cos()\n",
    "\n",
    "x = torch.zeros(5, requires_grad=True)\n",
    "out, bw_callback = explicit_autograd(f)(x)\n",
    "print(out)\n",
    "print(bw_callback.values)\n",
    "# the saved activations can be freely manipulated\n",
    "bw_callback.values = tree_map(lambda x: x.to('cpu'), bw_callback.values)\n",
    "bw_callback.values = tree_map(lambda x: x.to('cuda'), bw_callback.values)\n",
    "\n",
    "gradOut = torch.ones(5)\n",
    "gradX, = bw_callback(gradOut)\n",
    "f(x).sum().backward()\n",
    "assert torch.allclose(x.grad, gradX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS=4\n",
    "# device_mesh = example_mesh(hosts=1, gpus=NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_layer(w, x):\n",
    "    return torch.mm(x, w)\n",
    "\n",
    "def initialize(num_gpus, stages_per_gpu, hidden_dim=4, local_batch_size=4, use_real=False):\n",
    "    if use_real:\n",
    "        device_mesh = example_mesh(hosts=1, gpus=num_gpus)\n",
    "    else:\n",
    "        device_mesh = simulator_mesh(hosts=1, gpus=num_gpus)\n",
    "    meshes = [device_mesh(gpu=i) for i in range(num_gpus)]\n",
    "    layers = [[] for _ in range(num_gpus)]\n",
    "    for idx, mesh in enumerate(meshes):\n",
    "        with active_mesh(mesh):\n",
    "            for _ in range(stages_per_gpu):\n",
    "                layers[idx].append(torch.randn(hidden_dim, hidden_dim, requires_grad=True))\n",
    "\n",
    "    def dataloader():\n",
    "        with active_mesh(meshes[0]):\n",
    "            # a great dataloader\n",
    "            return torch.randn(local_batch_size, hidden_dim, requires_grad=True)\n",
    "\n",
    "    return device_mesh, meshes, layers, dataloader\n",
    "    \n",
    "def fill_drain(num_microbatches, meshes, layers, dataloader):\n",
    "    pp_stages = len(meshes)\n",
    "    bw_callbacks = {}\n",
    "    bw_microbatches = []\n",
    "    for mb_num in range(num_microbatches):\n",
    "        mb = dataloader() # loads the microbatch\n",
    "        with set_meta(str(mb_num)):\n",
    "            for stage in range(0, pp_stages):\n",
    "                with active_mesh(meshes[stage]):\n",
    "                    mb = mb.to_mesh(meshes[stage]) # moves the microbatch to the current stage\n",
    "                    for layer_idx, layer in enumerate(layers[stage]):\n",
    "                        mb, bw_callback = explicit_autograd(call_layer)(layer, mb) # calls the current stage's layer\n",
    "                        bw_callbacks[(mb_num, stage, layer_idx)] = bw_callback # saves bw callback for later\n",
    "        bw_microbatches.append(mb)\n",
    "        \n",
    "    for mb_num, mb in enumerate(bw_microbatches):\n",
    "        with set_meta(str(mb_num)):\n",
    "            # Iterate from the back to the front\n",
    "            for stage in range(pp_stages - 1, -1, -1):\n",
    "                # Get the corresponding bw_callback\n",
    "                with active_mesh(meshes[stage]):\n",
    "                    for layer_idx in range(len(layers[stage]) -1, -1, -1):\n",
    "                        gradWeight, mb = bw_callbacks[(mb_num, stage, layer_idx)](mb)\n",
    "                        del bw_callbacks[(mb_num, stage, layer_idx)]\n",
    "                    if stage != 0:\n",
    "                        mb = mb.to_mesh(meshes[stage - 1])\n",
    "\n",
    "def fill_drain_interleaved(num_microbatches, meshes, layers, dataloader):\n",
    "    pp_stages = len(meshes)\n",
    "    bw_callbacks = {}\n",
    "    bw_microbatches = []\n",
    "    stages_per_layer = len(layers[0])\n",
    "    microbatches = []\n",
    "    for loop in range(stages_per_layer):\n",
    "        for mb_num in range(num_microbatches):\n",
    "            for stage in range(0, pp_stages):\n",
    "                with set_meta(str(mb_num)):\n",
    "                    if loop == 0:\n",
    "                        microbatches.append(dataloader())\n",
    "                    mb = microbatches[mb_num]\n",
    "                    with active_mesh(meshes[stage]):\n",
    "                        mb = mb.to_mesh(meshes[stage])\n",
    "                        layer = layers[stage][loop]\n",
    "                        mb, bw_callback = explicit_autograd(call_layer)(layer, mb)\n",
    "                        bw_callbacks[(mb_num, stage, loop)] = bw_callback # saves bw callback for later\n",
    "                        # if stage == pp_stages - 1:\n",
    "                        #     mb = mb.to_mesh(meshes[0])\n",
    "                        # elif stage == pp_stages -1 and loop == stages_per_layer - 1:\n",
    "                        #     pass\n",
    "                        # else:\n",
    "                        #     mb = mb.to_mesh(meshes[stage + 1])\n",
    "                        microbatches[mb_num] = mb\n",
    "    return                   \n",
    "    for loop in range(stages_per_layer - 1, -1, -1):\n",
    "        for mb_num, mb in enumerate(bw_microbatches):\n",
    "            with set_meta(str(mb_num)):\n",
    "                # Iterate from the back to the front\n",
    "                for stage in range(pp_stages - 1, -1, -1):\n",
    "                    # Get the corresponding bw_callback\n",
    "                    with active_mesh(meshes[stage]):\n",
    "                        mb = mb.to_mesh(meshes[stage])\n",
    "                        gradWeight, mb = bw_callbacks[(mb_num, stage, loop)](mb)\n",
    "                        del bw_callbacks[(mb_num, stage, loop)]\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controller.simulator import simulate_commands, chrome_events, visualize_events, analyze_events\n",
    "device_mesh, meshes, layers, dataloader = initialize(num_gpus=4, stages_per_gpu=2, local_batch_size=32)\n",
    "fill_drain(num_microbatches=4, meshes=meshes, layers=layers, dataloader=dataloader)\n",
    "events = simulate_commands(device_mesh.client.backend.worker_commands)\n",
    "visualize_events(events)\n",
    "analyze_events(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "\n",
    "# Create a dropdown widget with the possible values\n",
    "make_range = lambda name, values: widgets.SelectionSlider(\n",
    "    options=values,\n",
    "    description=f'{name}:',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "def update(num_gpus, batch_size_per_gpu, use_real=False):\n",
    "    num_layers = 8\n",
    "    stages_per_gpu = 8 // num_gpus\n",
    "    num_microbatches = num_gpus * batch_size_per_gpu\n",
    "    print(f\"Stages per GPU: {stages_per_gpu}\")\n",
    "    print(f\"batch size for pipeline: {batch_size_per_gpu * num_gpus}\")\n",
    "    device_mesh, meshes, layers, dataloader = initialize(num_gpus=num_gpus, stages_per_gpu=stages_per_gpu, local_batch_size=1, use_real=use_real)\n",
    "    fill_drain(num_microbatches=num_microbatches, meshes=meshes, layers=layers, dataloader=dataloader)\n",
    "    if not use_real:\n",
    "        events = simulate_commands(device_mesh.client.backend.worker_commands)\n",
    "        visualize_events(events)\n",
    "        analyze_events(events)\n",
    "\n",
    "interact(update, num_gpus = make_range('num gpus', [2,4,8]), batch_size_per_gpu = IntSlider(value=1, min=1, max=10, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(num_gpus=8, batch_size_per_gpu=4, use_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
